{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General \n",
    "import os\n",
    "\n",
    "# PDFs\n",
    "import pdfplumber\n",
    "import json\n",
    "import regex as re\n",
    "\n",
    "# LLMs\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import tiktoken\n",
    "\n",
    "# Neo4j\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j.exceptions import AuthError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating GPT & Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the API key and model name\n",
    "MODEL=\"gpt-4o-mini\"\n",
    "\n",
    "# Load OpenAI API Key from requirements file\n",
    "with open(\"gpt-personal-key.txt\", \"r\") as file:\n",
    "    OPENAI_API_KEY = file.read().strip()\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j Connection Setup\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\"\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "try:\n",
    "    # Test the connection\n",
    "    with driver.session() as session:\n",
    "        session.run(\"RETURN 1\")\n",
    "    print(\"Connected to Neo4j successfully.\")\n",
    "except AuthError as e:\n",
    "    print(\"Authentication failed. Check your credentials:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_neo4j_database():\n",
    "    \"\"\"Delete all nodes and relationships in the Neo4j database.\"\"\"\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    print(\"Neo4j database cleared successfully.\")\n",
    "\n",
    "# Run the function to clear the database\n",
    "clear_neo4j_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Railway Accident\n",
      "Investigation Unit\n",
      "Ireland\n",
      "INVESTIGATION REPORT\n",
      "Collision between a car and a train at\n",
      "Level Crossing XM190, Mayo, 9th September 2023\n",
      "RAIU Investigation Report No: 2024-R003\n",
      "Published: 12/12/2024\n",
      "Collision between a car and a train at Level Crossing XM190, Mayo, 9th September 2023\n",
      "Report Description\n",
      "Report publication\n",
      "This report is published by the Railway Accident Investigation Unit (RAIU). The copyright in\n",
      "the enclosed report remains with the RAIU by virtue of Regulation 9 (7\n"
     ]
    }
   ],
   "source": [
    "# Extract the text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Test PDF extraction\n",
    "pdf_text = extract_text_from_pdf(\"raiu_example_collision.pdf\")\n",
    "print(pdf_text[:500])  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processed Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1:\n",
      "Railway Accident\n",
      "Investigation Unit\n",
      "Ireland\n",
      "INVESTIGATION REPORT\n",
      "Collision between a car and a train at\n",
      "Level Crossing XM190, Mayo, 9th September 2023\n",
      "RAIU Investigation Report No: 2024-R003\n",
      "Published: 12/12/2024\n",
      "----------------------------------------\n",
      "Page 2:\n",
      "Collision between a car and a train at Level Crossing XM190, Mayo, 9th September 2023\n",
      "Report Description\n",
      "Report publication\n",
      "This report is published by the Railway Accident Investigation Unit (RAIU). The copyright in\n",
      "the enclosed report remains with the RAIU by virtue of Regulation 9 (7) of European Union\n",
      "(EU) (Railway Safety) (Reporting and Investigation of Serious Accidents, Accidents and\n",
      "Incidents) Regulations 2020 (S.I. 430 of 2020). No person may produce, reproduce or transmit\n",
      "in any form o\n",
      "----------------------------------------\n",
      "Page 3:\n",
      "Collision between a car and a train at Level Crossing XM190, Mayo, 9th September 2023\n",
      "Preface\n",
      "The RAIU is an independent investigation unit within the Department of Transport which\n",
      "conducts investigations into accidents and incidents on the national railway network including\n",
      "the Dublin Area Rapid Transit (DART) network, the Luas light rail system, heritage and\n",
      "industrial railways in Ireland. Investigations are carried out in accordance with the Railway\n",
      "Safety Directive (EU) 2016/798 enshrined in\n",
      "----------------------------------------\n",
      "Page 4:\n",
      "Collision between a car and a train at Level Crossing XM190, Mayo, 9th September 2023\n",
      "Summary\n",
      "At approximately 15:15 hours (hrs) on Saturday the 9th September 2023, the Iarnród Éireann\n",
      "(IÉ) 12:45 hrs Heuston to Westport passenger service (Train A804) was approaching\n",
      "Prendergast’s Level Crossing (LC), asset number XM190 (to be referred to as LC XM190),\n",
      "located between Ballyhaunis and Claremorris (County Mayo), at a speed of 70 miles per hour\n",
      "(mph) (110 kilometres per hour (km/h)). The train drive\n",
      "----------------------------------------\n",
      "Page 5:\n",
      "Collision between a car and a train at Level Crossing XM190, Mayo, 9th September 2023\n",
      "The car sustained substantial damage on impact and was propelled approximately 31 metres\n",
      "(m) into an adjacent field landing on the passenger side.\n",
      "The front of Train A804 came to a stop 310 m past the centre of LC XM190.\n",
      "Driver A804 contacted the Mayo Line Signalman requesting emergency service and followed\n",
      "all other post-accident procedures correctly.\n",
      "The two occupants of the car sustained injuries (the passen\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF while allowing for pre-processing.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text.append(page_text)\n",
    "    \n",
    "    return text  # Returns a list where each item is a page's text\n",
    "\n",
    "# Extract pages as a list\n",
    "pdf_pages = extract_text_from_pdf(\"raiu_example_collision.pdf\")\n",
    "\n",
    "# Print the first few pages to inspect where the TOC might be\n",
    "for i, page in enumerate(pdf_pages[:5]):  # Check first 5 pages\n",
    "    print(f\"Page {i+1}:\\n{page[:500]}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Railway Accident\n",
      "Investigation Unit\n",
      "Ireland\n",
      "INVESTIGATION REPORT\n",
      "Collision between a car and a train at\n",
      "Level Crossing XM190, Mayo, 9th September 2023\n",
      "RAIU Investigation Report No: 2024-R003\n",
      "Published: 12/12/2024\n",
      "Collision between a car and a train at Level Crossing XM190, Mayo, 9th September 2023\n",
      "Report Description\n",
      "Report publication\n",
      "This report is published by the Railway Accident Investigation Unit (RAIU). The copyright in\n",
      "the enclosed report remains with the RAIU by virtue of Regulation 9 (7) of European Union\n",
      "(EU) (Railway Safety) (Reporting and Investigation of Serious Accidents, Accidents and\n",
      "Incidents) Regulations 2020 (S.I. 430 of 2020). No person may produce, reproduce or transmit\n",
      "in any form or by any means this report or any part thereof without the express permission of\n",
      "the RAIU. This report may be freely used for educational purposes.\n",
      "Where the report has been altered following its original publication, details on the changes will\n",
      "be given.\n",
      "Report structure\n",
      "The report structure is written as close as possible to the structure set out in the “Commission\n",
      "Implementation Regulation (EU) 2020/572 of 24 April 2020 on the reporting structure to be\n",
      "followed for railway accident and incident investigation reports” having regard to “Directive\n",
      "(EU) 2016/798 of the European Parliament and of the Council of 11 May 2016 on railway\n",
      "safety”.\n",
      "Reader guide\n",
      "All dimensions and speeds in this report are given using the International System of Units (SI\n",
      "Units). Where the normal railway practice, in some railway organisations, is to use imperial\n",
      "dimensions; imperial dimensions are used, and the SI Unit is also given.\n",
      "All abbreviations and technical terms (which appear in italics the first time they appear in the\n",
      "report) are explained in the glossary.\n",
      "Descriptions and figures may be simplified in order to illustrate concepts to non-technical\n",
      "readers.\n",
      "Further information\n",
      "For further information, or to contact the RAIU, please see details below:\n",
      "RAIU email: info@raiu.ie\n",
      "2nd Floor, 2 Leeson Lane website: www.raiu.ie\n",
      "Dublin 2, Ireland. telephone: + 353 1 604 1050\n",
      "Railway Accident Investigation Unit i\n",
      "Collision between a car and a train at Level Crossing XM190, Mayo, 9th September 2023\n",
      "Preface\n",
      "The RAIU is an independent investigation unit within the Department of Transport which\n",
      "conducts investigations into accidents and incidents on the national railway network including\n",
      "the Dublin Area Rapid Transit (DART) network, the Luas light rail system, heritage and\n",
      "industrial railways in Ireland. Investigations are carried out in accordance with the Railway\n",
      "Safety Directive (EU) 2016/798 enshrined in the European Union (Railway Safety) (Reporting\n",
      "and Investigation of Serious Accidents, Accidents and Incidents) Regulations 2020; and,\n",
      "where relevant, by the application of the Railway Safety (Reporting and Investigation of\n",
      "Serious Accidents, Accidents and Incidents Involving Certain Railways) Act 2020.\n",
      "The RAIU investigate all serious accidents. A serious accident means any train collision or\n",
      "derailment of trains, resulting in the death of at least one person or serious injuries to five or\n",
      "more persons or extensive damage to rolling stock, the infrastructure or the environment, and\n",
      "any other similar accident with an obvious impact on railway or tramline safety regulation or\n",
      "the management of safety. During an investigation, if the RAIU make some early findings on\n",
      "safety issues that require immediate action, the RAIU will issue an Urgent Safety Advice\n",
      "Notice outlining the associated safety recommendation(s); other issues may require a Safety\n",
      "Advice Notice.\n",
      "The RAIU may investigate and report on accidents and incidents which under slightly different\n",
      "conditions may have led to a serious accident.\n",
      "The RAIU may also carry out trend investigations where the occurrence is part of a group of\n",
      "related occurrences that may or may not have warranted an investigation as individual\n",
      "occurrences, but the apparent trend warrants investigation.\n",
      "The RAIU investigation shall analyse the established facts and findings (i.e. performance of\n",
      "operators, rolling stock and/or technical installations) which caused the occurrence. The\n",
      "analyses shall then lead to the identification of the safety critical factors that caused or\n",
      "otherwise contributed to the occurrence, including facts identified as precursors. An accident\n",
      "or incident may be caused by causal, contributing and systemic factors which are equally\n",
      "important and should be consider during the RAIU investigation. From this, the RAIU may\n",
      "make safety recommendations in order to prevent accidents and incidents in the future and\n",
      "improve railway safety.\n",
      "It is not the purpose of an RAIU investigation to attribute blame or liability.\n",
      "Railway Accident Investigation Unit ii\n",
      "Collision between a car and a train at Level Crossing XM190, Mayo, 9th September 2023\n",
      "Summary\n",
      "At approximately 15:15 hours (hrs) on Saturday the 9th September 2023, the Iarnród Éireann\n",
      "(IÉ) 12:45 hrs Heuston to Westport passenger service (Train A804) was approaching\n",
      "Prendergast’s Level Crossing (LC), asset number XM190 (to be referred to as LC XM190),\n",
      "located between Ballyhaunis and Claremorris (County Mayo), at a speed of 70 miles per hour\n",
      "(mph) (110 kilometres per hour (km/h)). The train driver (Driver A804) sounded the horn at the\n",
      "whistle board associated with LC XM190 on their approach.\n",
      "LC XM190 is an Occupational Public (OP) user worked unattended level crossing (UWLC)\n",
      "meaning it is guarded by metal gates across a public road; whereby a member of the public,\n",
      "the “user”, will have to open and shut the gates to cross the railway and continue on the road.\n",
      "At the same time as Train A804 was approaching LC XM190 a car was also approaching LC\n",
      "XM190 from the up side (right hand side from the perspective of Driver A804). The car was\n",
      "travelling on a rural local road L65516 which links national road, N60, with another local road,\n",
      "L5551, and onto the N17. The speed limit for the local road is 80 km/h. The driver of the car\n",
      "(Car Driver) had taken a wrong turn at Claremorris and their satellite navigation system had\n",
      "diverted them onto these local roads which routed them over LC XM190 to continue on their\n",
      "journey.\n",
      "On the approach to LC XM190 there are three “Level Crossing with No Flashing Red Signal\n",
      "(with Barriers or Gates)” advance warning signs (Sign W121) located at 100 metres (m), 200\n",
      "m and 300 m in advance of LC XM190. In addition, there is a “Warning Railway Crossing\n",
      "Ahead Stop before you Cross the Railway” sign, a mandatory Stop Sign (Sign RUS 027), and\n",
      "a “Warning Trains” sign mounted on poles at LC XM190.\n",
      "The gates at LC XM190 were left open to road traffic by a previous unknown user of LC\n",
      "XM190.\n",
      "When Driver A804 saw the car approaching LC XM190 they sounded the train horn again.\n",
      "Driver A804 could see that the car was travelling “a bit fast” and made a full service brake\n",
      "application and continued to sound the horn. On realising the car was not going to stop, Driver\n",
      "A804 made an emergency brake application.\n",
      "As Train A804 slowed, Driver A804 saw that the car was also slowing while arriving onto LC\n",
      "XM190, with the car coming to a “standstill” on the railway line.\n",
      "There was insufficient time to bring Train A804 to a stop before reaching LC XM190 and Train\n",
      "A804 collided with the car (at the time of the collision the coupler was in the extended position\n",
      "as a result of issues with retracting couplers on the Intercity Railcar (ICR Fleet)).\n",
      "Railway Accident Investigation Unit iii\n",
      "Collision between a car and a train at Level Crossing XM190, Mayo, 9th September 2023\n",
      "The car sustained substantial damage on impact and was propelled approximately 31 metres\n",
      "(m) into an adjacent field landing on the passenger side.\n",
      "The front of Train A804 came to a stop 310 m past the centre of LC XM190.\n",
      "Driver A804 contacted the Mayo Line Signalman requesting emergency service and followed\n",
      "all other post-accident procedures correctly.\n",
      "The two occupants of the car sustained injuries (the passenger sustained life-changing\n",
      "injuries) and were treated at the scene before been airlifted to hospital for treatment.\n",
      "The RAIU have identified the following causal factors (CaF) relating to the collision of Train\n",
      "A804 with a car at LC XM190, as follows:\n",
      "• CaF-01 – The gates at LC XM190 were left open by a previous user;\n",
      "• CaF-02 – The Car Driver was unfamiliar with Occupational Public (OP) Type level\n",
      "crossings and as a result the Car Driver did not:\n",
      "• React to the three advance warning signs (Sign W 121) on approach to LC XM190 by\n",
      "slowing the car;\n",
      "• Obey the instructions listed in the “Danger Live Railway Crossing” sign at LC XM190;\n",
      "• Stop at the Stop Sign or Stop Line to look for approaching trains as required by Road\n",
      "Safety Authority’s (RSA) Rules of the Road.\n",
      "The following may have been a contributory factor (CoF) to the damage and injuries sustained\n",
      "to the car occupants and car:\n",
      "• CoF-01 – The coupler was in the extended position (as a result of issues related to\n",
      "retracting the coupler). Had the coupler been retracted, it may have reduced the rate of\n",
      "rotation of the car from the initial impact and may have reduced the damage sustained by\n",
      "the car and the subsequent injuries to the car occupants. However, it cannot be\n",
      "determined, what damage the car would have sustained had the coupler been retracted\n",
      "(i.e. there could have been worse damage; and in addition, there could have been other\n",
      "unintended unwanted consequences).\n",
      "The RAIU have identified the following likely systemic factor (SF) to the accident:\n",
      "• SF-01 – SF-01 - Sign W 121 does not portray clear meaning that the user is approaching\n",
      "a UWLC, a hazard (i.e. live railway) and does not indicate the severity of not adhering to\n",
      "the warning (i.e. being struck by a train).\n",
      "Railway Accident Investigation Unit iv\n",
      "Collision between a car and a train at Level Crossing XM190, Mayo, 9th September 2023\n",
      "At the time of the accident, a Decision Support System (DSS), which provides information for\n",
      "users on the approach of trains, was present at LC XM\n"
     ]
    }
   ],
   "source": [
    "def extract_text_omit_toc(pdf_path, toc_start=7, toc_end=9):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF while skipping the Table of Contents.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            if toc_start <= i+1 <= toc_end:  # Skip TOC pages\n",
    "                continue\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text.append(page_text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Extract text without TOC\n",
    "filtered_pdf_pages = extract_text_omit_toc(\"raiu_example_collision.pdf\")\n",
    "\n",
    "# Join pages into a single text document\n",
    "cleaned_text = \"\\n\".join(filtered_pdf_pages)\n",
    "print(cleaned_text[:10000])  # Preview the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Railway Accident\n",
      "Investigation Unit\n",
      "Ireland\n",
      "INVESTIGATION REPORT\n",
      "Collision between a car and a train at\n",
      "Level Crossing XM190, Mayo, 9th September 2023\n",
      "RAIU Investigation Report No: 2024-R003\n",
      "Published: 12/12/2024\n",
      "Report Description\n",
      "Report publication\n",
      "This report is published by the Railway Accident Investigation Unit (RAIU). The copyright in\n",
      "the enclosed report remains with the RAIU by virtue of Regulation 9 (7) of European Union\n",
      "(EU) (Railway Safety) (Reporting and Investigation of Serious Accidents, Accidents and\n",
      "Incidents) Regulations 2020 (S.I. 430 of 2020). No person may produce, reproduce or transmit\n",
      "in any form or by any means this report or any part thereof without the express permission of\n",
      "the RAIU. This report may be freely used for educational purposes.\n",
      "Where the report has been altered following its original publication, details on the changes will\n",
      "be given.\n",
      "Report structure\n",
      "The report structure is written as close as possible to the structure set out in the “Commission\n",
      "Imp\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing headers, footers, and empty lines.\n",
    "    \"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if re.match(r'^(Page \\d+|Collision between a car and a train at Level Crossing XM190, Mayo, 9th September 2023|Railway Accident Investigation Unit)$', line):\n",
    "            continue\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "# Apply cleaning\n",
    "final_cleaned_text = clean_text(cleaned_text)\n",
    "print(final_cleaned_text[:1000])  # Preview cleaned text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Chunk Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 88\n"
     ]
    }
   ],
   "source": [
    "def split_text_into_chunks(text, chunk_size=2000, chunk_overlap=300):\n",
    "    \"\"\"\n",
    "    Splits text into smaller overlapping chunks using LangChain's text splitter.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Split the extracted text\n",
    "text_chunks = split_text_into_chunks(final_cleaned_text)\n",
    "\n",
    "# Print the number of chunks and a sample chunk\n",
    "print(f\"Total Chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant Chunk Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword-based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_relevant_chunks(chunks, keyword):\n",
    "#     \"\"\"\n",
    "#     Returns chunks that contain a specific keyword.\n",
    "#     \"\"\"\n",
    "#     relevant_chunks = [chunk for chunk in chunks if keyword.lower() in chunk.lower()]\n",
    "#     return relevant_chunks\n",
    "\n",
    "# # Example: Find chunks mentioning \"location\"\n",
    "# location_chunks = find_relevant_chunks(text_chunks, \"location\")\n",
    "\n",
    "# print(f\"Found {len(location_chunks)} relevant chunks.\")\n",
    "# print(\"Sample Chunk:\\n\", location_chunks[0] if location_chunks else \"No relevant chunks found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Miniconda\\envs\\master_thesis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 88 chunks in FAISS.\n"
     ]
    }
   ],
   "source": [
    "# Convert text chunks into FAISS vector store\n",
    "vectorstore = FAISS.from_texts(text_chunks, embeddings)\n",
    "\n",
    "print(f\"Stored {len(text_chunks)} chunks in FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_relevant_chunks(entities, top_k=1):\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for each entity of interest\n",
    "    using FAISS similarity search and removes duplicates.\n",
    "    \n",
    "    Args:\n",
    "    - entities (list): List of entity names to query (e.g., [\"date\", \"location\", \"regulatory_body\"])\n",
    "    - top_k (int): Number of chunks to retrieve per entity\n",
    "    \n",
    "    Returns:\n",
    "    - unique_relevant_chunks (list): Deduplicated relevant chunks\n",
    "    \"\"\"\n",
    "    retrieved_chunks = set()  # Use a set to avoid duplicate chunks\n",
    "\n",
    "    for entity in entities:\n",
    "        print(f\"Searching for entity: {entity}\")\n",
    "        query = f\"Information about {entity}.\"\n",
    "        found_chunks = vectorstore.similarity_search(query, k=top_k)\n",
    "\n",
    "        for chunk in found_chunks:\n",
    "            retrieved_chunks.add(chunk.page_content)  # Add chunk if not already present\n",
    "\n",
    "    # Convert set back to a list and join into a single string\n",
    "    unique_relevant_chunks = list(retrieved_chunks)\n",
    "    combined_text = \"\\n\".join(unique_relevant_chunks)\n",
    "\n",
    "    print(f\"Found {len(unique_relevant_chunks)} unique relevant chunks.\")\n",
    "    return combined_text\n",
    "\n",
    "# Define entities of interest\n",
    "entities_of_interest = [\"date\", \"location\", \"regulatory_body\"]\n",
    "\n",
    "# Find & combine relevant chunks\n",
    "relevant_text = find_most_relevant_chunks(entities_of_interest, top_k=1)\n",
    "\n",
    "# print(f\"Most Relevant Chunks Combined:\\n{relevant_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in a given text for a specified OpenAI model.\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoder.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your entities of interest are: \n",
      "['date', 'location', 'regulatory_body']\n",
      "\n",
      "Your schema example is:\n",
      "{'date': '2023-02-22', 'location': 'Emly, County Tipperary', 'regulatory_body': 'Railway Accident Investigation Unit'}\n"
     ]
    }
   ],
   "source": [
    "schema_example = {\n",
    "        \"date\": \"2023-02-22\",\n",
    "        \"location\": \"Emly, County Tipperary\",\n",
    "        \"regulatory_body\": \"Railway Accident Investigation Unit\"\n",
    "        }\n",
    "\n",
    "print(f\"Your entities of interest are: \\n{entities_of_interest}\\n\")\n",
    "print(f\"Your schema example is:\\n{schema_example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(text):\n",
    "    \"\"\"\n",
    "    Constructs the entity extraction prompt.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    Extract key entities from the following accident report.\n",
    "\n",
    "    Provide the output in valid JSON format with categories {entities_of_interest}. Ensure that the response is only valid JSON and \n",
    "    contains no other text or formatting. Here's an example for you to follow:\n",
    "    {schema_example}\n",
    "    If you cannot extract anything, please provide an empty JSON object.\n",
    "\n",
    "    Here is the text to analyze: {text}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(prompt, temperature=0.3):\n",
    "    \"\"\"\n",
    "    Calls the GPT model with the structured prompt and returns the raw response.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in analyzing railway accident reports. Return output in JSON format only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    response_text = completion.choices[0].message.content.strip()\n",
    "    \n",
    "    # Remove markdown JSON formatting if present\n",
    "    response_text = re.sub(r'^```json\\n?|```$', '', response_text).strip()\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text, token_limit=4096):\n",
    "    \"\"\"\n",
    "    Extracts key entities from an accident report using GPT.\n",
    "    - First, counts tokens and allows user decision.\n",
    "    - If within limit, runs GPT and handles errors.\n",
    "    \"\"\"\n",
    "    prompt = build_prompt(text)\n",
    "    token_count = count_tokens(prompt)\n",
    "\n",
    "    print(f\"Token Count for Prompt: {token_count} (Limit: {token_limit})\")\n",
    "    print(f\"Prompt Costs for model {MODEL}: ${token_count * 0.00000015}\")\n",
    "\n",
    "    # Allow user to decide if they want to proceed\n",
    "    if token_count > token_limit:\n",
    "        print(\"Token count is too high! Please reduce the chunk size or refine the prompt.\")\n",
    "        return None  # Stops execution here\n",
    "\n",
    "    # Confirm before making the API call\n",
    "    proceed = input(\"Do you want to proceed with extraction? (yes/no): \").strip().lower()\n",
    "    if proceed != \"yes\":\n",
    "        print(\"Extraction aborted by user.\")\n",
    "        return None  # Stops execution\n",
    "\n",
    "    print(\"Sending request to GPT...\")\n",
    "\n",
    "    response_text = call_gpt(prompt)\n",
    "\n",
    "    try:\n",
    "        return json.loads(response_text)  # Ensure valid JSON\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON:\", str(e))\n",
    "        print(\"Storing raw response for review...\")\n",
    "\n",
    "        # Save the faulty response for debugging\n",
    "        with open(\"failed_gpt_responses.json\", \"a\") as file:\n",
    "            json.dump({\"input_text\": text[:1000], \"raw_output\": response_text}, file, indent=4)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "        return {}  # Return empty dictionary in case of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count for Prompt: 1348 (Limit: 4096)\n",
      "Prompt Costs for model gpt-4o-mini: $0.00020219999999999998\n",
      "Sending request to GPT...\n",
      "Extracted Entities: {'date': '2023-09-09', 'location': 'level crossing XM190', 'regulatory_body': 'Railway Accident Investigation Unit'}\n"
     ]
    }
   ],
   "source": [
    "entities = extract_entities(relevant_text)\n",
    "\n",
    "print(\"Extracted Entities:\", entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"date\": \"2023-09-09\", \"location\": \"level crossing XM190\", \"regulatory_body\": \"Railway Accident Investigation Unit\"}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in Neo4j\n",
    "def store_in_neo4j(json_data):\n",
    "    \"\"\"Store extracted data in Neo4j.\"\"\"\n",
    "    if not json_data:\n",
    "        print(\"No valid entities to store in Neo4j.\")\n",
    "        return\n",
    "    \n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        for category, item in json_data.items():  # Iterate over key-value pairs\n",
    "            if isinstance(item, list):  # If it's a list, iterate over items\n",
    "                for value in item:\n",
    "                    session.run(\"\"\"\n",
    "                        MERGE (n:Entity {name: $name, category: $category})\n",
    "                    \"\"\", name=value, category=category)\n",
    "            else:  # If it's a single string, store it directly\n",
    "                session.run(\"\"\"\n",
    "                    MERGE (n:Entity {name: $name, category: $category})\n",
    "                \"\"\", name=item, category=category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in Neo4j with Relationships\n",
    "def store_in_neo4j(json_data):\n",
    "    \"\"\"Store extracted data in Neo4j and create relationships.\"\"\"\n",
    "    if not json_data:\n",
    "        print(\"No valid entities to store in Neo4j.\")\n",
    "        return\n",
    "    \n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        # Create nodes\n",
    "        session.run(\"\"\"\n",
    "            MERGE (d:Date {name: $date})\n",
    "        \"\"\", date=json_data.get(\"date\", \"Unknown\"))\n",
    "        \n",
    "        session.run(\"\"\"\n",
    "            MERGE (l:Location {name: $location})\n",
    "        \"\"\", location=json_data.get(\"location\", \"Unknown\"))\n",
    "        \n",
    "        session.run(\"\"\"\n",
    "            MERGE (r:RegulatoryBody {name: $regulatory_body})\n",
    "        \"\"\", regulatory_body=json_data.get(\"regulatory_body\", \"Unknown\"))\n",
    "        \n",
    "        # Create relationships\n",
    "        session.run(\"\"\"\n",
    "            MATCH (d:Date {name: $date}), (l:Location {name: $location})\n",
    "            MERGE (d)-[:OCCURRED_AT]->(l)\n",
    "        \"\"\", date=json_data.get(\"date\", \"Unknown\"), location=json_data.get(\"location\", \"Unknown\"))\n",
    "        \n",
    "        session.run(\"\"\"\n",
    "            MATCH (l:Location {name: $location}), (r:RegulatoryBody {name: $regulatory_body})\n",
    "            MERGE (l)-[:REGULATED_BY]->(r)\n",
    "        \"\"\", location=json_data.get(\"location\", \"Unknown\"), regulatory_body=json_data.get(\"regulatory_body\", \"Unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store extracted entities into Neo4j\n",
    "try:\n",
    "    db_result = store_in_neo4j(entity_json)\n",
    "    print(\"Data stored in Neo4j successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to store data in Neo4j:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Neo4j connection\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
