{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PDFs\n",
    "import pdfplumber\n",
    "\n",
    "# LLMs\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import textwrap\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.graphs.graph_document import (\n",
    "    GraphDocument,\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Neo4j\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j.exceptions import AuthError\n",
    "\n",
    "# Typing & Validation\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Raw Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Incident Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE-10375 - 210827 Collision with track equipment.pdf\n",
      "IE-10397 - 211207 Clontarf.pdf\n",
      "IE-10404 - 230222 Broken Rail Emly.pdf\n",
      "IE-200608 BnM Collision LC Offaly.pdf\n",
      "IE-6218-200111 Collision RRME Rosslare.pdf\n",
      "IE-6262-200429 LC Collision XM240.pdf\n",
      "IE-6291-200524 LC XA068 Ashfield.pdf\n",
      "IE-6305 - 200707_locomotive_224.pdf\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing PDFs\n",
    "pdf_directory = \"./reports_ie/\"\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [f for f in os.listdir(pdf_directory) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "for file in pdf_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file name that you want to process\n",
    "pdf_name = \"IE-6218-200111 Collision RRME Rosslare.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Page 4]\n",
      "Summary\n",
      "On 11th January 2020, T3 Possessions were granted between the 112 Mile Post (MP) 880\n",
      "yards and the 112 MP 1320 yards (Rosslare Strand to Rosslare Europort) for Iarnród Éireann’s\n",
      "Infrastructure Manager’s (IÉ-IM) Chief Civil Engineer’s (CCE) Department in relation to the\n",
      "Rosslare Coastal Erosion Project.\n",
      "The work being undertaken in the T3 Possessions included the erection of viewing distance\n",
      "marker boards (sometimes referred to as V Boards) on the north and south sides of a newly\n"
     ]
    }
   ],
   "source": [
    "# Define regex patterns to identify the start of the summary and contents sections\n",
    "SUMMARY_PATTERN = re.compile(r\"^summary\", re.IGNORECASE)\n",
    "CONTENTS_PATTERN = re.compile(r\"^contents\", re.IGNORECASE)\n",
    "\n",
    "# Define function to extract the summary section from Ireland reports\n",
    "def extract_summary_section(pdf_path: str, header_lines: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from 'Summary' to 'Contents' in an Irish rail report PDF.\n",
    "    If no summary section is found, returns the text from all pages.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"File not found: {pdf_path}\")\n",
    "\n",
    "    summary_text = \"\"\n",
    "    full_text = \"\"\n",
    "    capturing = False  \n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text_lines = page_text.split(\"\\n\")\n",
    "                text_without_header = text_lines[header_lines:]\n",
    "                page_content = f\"[Page {page.page_number}]\\n\" + \"\\n\".join(text_without_header) + \"\\n\\n\"\n",
    "\n",
    "                # Always append to full_text\n",
    "                full_text += page_content\n",
    "\n",
    "                if text_without_header:\n",
    "                    first_line = text_without_header[0].strip().lower()\n",
    "\n",
    "                    if SUMMARY_PATTERN.match(first_line):\n",
    "                        capturing = True\n",
    "                    \n",
    "                    if CONTENTS_PATTERN.match(first_line) and capturing:\n",
    "                        # Stop capturing when \"Contents\" is found after summary started.\n",
    "                        break\n",
    "                    \n",
    "                    if capturing:\n",
    "                        summary_text += page_content\n",
    "\n",
    "    if not summary_text:\n",
    "        print(f\"Warning: No summary section found in {pdf_path}. Returning full text.\")\n",
    "        return full_text\n",
    "\n",
    "    return summary_text\n",
    "\n",
    "# Example usage:\n",
    "pdf_text = extract_summary_section(f\"./reports_ie/{pdf_name}\", header_lines=1)\n",
    "print(pdf_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Accident Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Broader",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0220fca2-2e4d-4861-baae-790f7f2acf9d",
       "rows": [
        [
         "0",
         "A1",
         "A1",
         "Collisions",
         "A collision event falling within the A.1 sub-categories for which detailed information is not (yet) available.",
         "A"
        ],
        [
         "1",
         "A1-1 ",
         "A1.1 ",
         "Collision of train with a train/rail vehicle",
         "A front to front; front to end or a side collision between a part of a train and a part of another train or rail vehicle; or with shunting rolling stock.",
         "A1"
        ],
        [
         "2",
         "A1-2",
         "A1.2",
         "Collision of train with obstacle within the clearance gauge",
         "A collision between a part of a train and objects fixed or temporarily present on or near the track (except at level crossings if lost by a crossing vehicle or user); including collision with overhead contact lines.",
         "A1"
        ],
        [
         "3",
         "A1-3 ",
         "A1.3 ",
         "Collision of one or more rail vehicles with another rail vehicle",
         "Same as A1.1 but concerning more rail vehicles not forming a train.",
         "A1"
        ],
        [
         "4",
         "A1-4 ",
         "A1.4 ",
         "Collision of one or more rail vehicles with obstacle within the clearance gauge",
         "Same as A1.2 but concerning one or more rail vehicles not forming a train.",
         "A1"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Code</th>\n",
       "      <th>Name</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Broader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>A1</td>\n",
       "      <td>Collisions</td>\n",
       "      <td>A collision event falling within the A.1 sub-c...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1-1</td>\n",
       "      <td>A1.1</td>\n",
       "      <td>Collision of train with a train/rail vehicle</td>\n",
       "      <td>A front to front; front to end or a side colli...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1-2</td>\n",
       "      <td>A1.2</td>\n",
       "      <td>Collision of train with obstacle within the cl...</td>\n",
       "      <td>A collision between a part of a train and obje...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1-3</td>\n",
       "      <td>A1.3</td>\n",
       "      <td>Collision of one or more rail vehicles with an...</td>\n",
       "      <td>Same as A1.1 but concerning more rail vehicles...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1-4</td>\n",
       "      <td>A1.4</td>\n",
       "      <td>Collision of one or more rail vehicles with ob...</td>\n",
       "      <td>Same as A1.2 but concerning one or more rail v...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   Code                                               Name  \\\n",
       "0     A1     A1                                         Collisions   \n",
       "1  A1-1   A1.1        Collision of train with a train/rail vehicle   \n",
       "2   A1-2   A1.2  Collision of train with obstacle within the cl...   \n",
       "3  A1-3   A1.3   Collision of one or more rail vehicles with an...   \n",
       "4  A1-4   A1.4   Collision of one or more rail vehicles with ob...   \n",
       "\n",
       "                                          Definition Broader  \n",
       "0  A collision event falling within the A.1 sub-c...       A  \n",
       "1  A front to front; front to end or a side colli...      A1  \n",
       "2  A collision between a part of a train and obje...      A1  \n",
       "3  Same as A1.1 but concerning more rail vehicles...      A1  \n",
       "4  Same as A1.2 but concerning one or more rail v...      A1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load category A events\n",
    "cat_a_events = pd.read_csv(\"category-a-event-types-source.csv\", encoding='latin-1')\n",
    "cat_a_events.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Langchain Chunk Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Incident Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 4\n",
      "First chunk:\n",
      "[Page 4]\n",
      "Summary\n",
      "On 11th January 2020, T3 Possessions were granted between the 112 Mile Post (MP) 880\n",
      "yards and the 112 MP 1320 yards (Rosslare Strand to Rosslare Europort) for Iarnród Éireann’s\n",
      "Infrastructure Manager’s (IÉ-IM) Chief Civil Engineer’s (CCE) Department in relation to the\n",
      "Rosslare Coastal Erosion Project.\n",
      "The work being undertaken in the T3 Possessions included the erection of viewing distance\n",
      "marker boards (sometimes referred to as V Boards) on the north and south sides of a newly\n",
      "re-opened level crossing, adjacent to Hayesland IÉ-IM compound. There were four members\n",
      "of CCE staff, working in pairs, erecting the V Boards on the north and south sides of the level\n",
      "crossing. The pair working on the south side included the Person In Charge Of Possession\n",
      "(PICOP)/ Engineering Supervisor (PICOP/ES) and the other pair, working on the north side,\n",
      "included the Acting Permanent Way Inspector (APWI).\n",
      "The APWI decided to transport the V Boards using a piece of wheeled rail-mounted\n",
      "maintenance equipment (RMME) at the level crossing. On arrival at the V Board erection\n",
      "location, it was discovered that the poles to hold the V Boards were too short and needed to\n",
      "be extended; this could be done by a welder working in Hayesland compound. Therefore, the\n",
      "APWI made the decision to put the V Boards back on the RMME and return to the Hayesland\n",
      "compound where the welder could extend the poles to the correct length.\n",
      "On arrival at level crossing (adjacent to Hayesland compound), APWI and another permanent\n",
      "way worker carried one pole each to the welder, leaving the RMME on the track. As APWI\n",
      "was explaining the requirements to the welder his mobile phone rang and he took the call. On\n",
      "completion of the call, the PICOP/ES asked the APWI if the line was clear for the possession\n",
      "to be handed back; the APWI responded that it was clear; intending to remove the RMME.\n",
      "The APWI continued his conversation with the welder, forgetting to remove the RMME.\n"
     ]
    }
   ],
   "source": [
    "# Define function to split the incident report into chunks\n",
    "def split_report_into_chunks(text: str, chunk_size: int = 2000, chunk_overlap: int = 300) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits text into smaller overlapping chunks using LangChain's text splitter.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        print(\"Warning: No text provided for splitting.\")\n",
    "        return []\n",
    "\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]  # If text is smaller than chunk size, return as single chunk\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Split the extracted text\n",
    "report_chunks = split_report_into_chunks(pdf_text)\n",
    "\n",
    "# Print the number of chunks and a sample chunk\n",
    "print(f\"Total chunks: {len(report_chunks)}\\nFirst chunk:\\n{report_chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Accident Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Id': 'A1',\n",
       "  'Code': 'A1',\n",
       "  'Name': 'Collisions',\n",
       "  'Definition': 'A collision event falling within the A.1 sub-categories for which detailed information is not (yet) available.',\n",
       "  'Broader': 'A'},\n",
       " {'Id': 'A1-1 ',\n",
       "  'Code': 'A1.1 ',\n",
       "  'Name': 'Collision of train with a train/rail vehicle',\n",
       "  'Definition': 'A front to front; front to end or a side collision between a part of a train and a part of another train or rail vehicle; or with shunting rolling stock.',\n",
       "  'Broader': 'A1'},\n",
       " {'Id': 'A1-2',\n",
       "  'Code': 'A1.2',\n",
       "  'Name': 'Collision of train with obstacle within the clearance gauge',\n",
       "  'Definition': 'A collision between a part of a train and objects fixed or temporarily present on or near the track (except at level crossings if lost by a crossing vehicle or user); including collision with overhead contact lines.',\n",
       "  'Broader': 'A1'},\n",
       " {'Id': 'A1-3 ',\n",
       "  'Code': 'A1.3 ',\n",
       "  'Name': 'Collision of one or more rail vehicles with another rail vehicle',\n",
       "  'Definition': 'Same as A1.1 but concerning more rail vehicles not forming a train.',\n",
       "  'Broader': 'A1'},\n",
       " {'Id': 'A1-4 ',\n",
       "  'Code': 'A1.4 ',\n",
       "  'Name': 'Collision of one or more rail vehicles with obstacle within the clearance gauge',\n",
       "  'Definition': 'Same as A1.2 but concerning one or more rail vehicles not forming a train.',\n",
       "  'Broader': 'A1'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the DataFrame to a list of rows in dictionary format\n",
    "cat_a_events_dict = cat_a_events.to_dict(\"records\")\n",
    "cat_a_events_dict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 32\n",
      "First three chunks:\n",
      "[['Accident Type: Collisions'], ['Accident Type: Collision of train with a train/rail vehicle'], ['Accident Type: Collision of train with obstacle within the clearance gauge']]\n"
     ]
    }
   ],
   "source": [
    "# Define function to split the event data into chunks\n",
    "def split_events_into_chunks(data: list[dict], chunk_size: int = 2000, chunk_overlap: int = 300) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Converts each dictionary row into a string and splits each string into smaller overlapping chunks.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"Warning: No data provided for splitting.\")\n",
    "        return []\n",
    "\n",
    "    # Prepare the list of formatted strings (chunks) from the dictionary rows\n",
    "    chunk_list = [\n",
    "        # f\"Id: {row['Id']} Code: {row['Code']} Name: {row['Name']} Definition: {row['Definition']} Broader: {row['Broader']}\"\n",
    "        f\"Accident Type: {row['Name']}\"\n",
    "        for row in data\n",
    "    ]\n",
    "\n",
    "    # Function to split a single text into smaller chunks\n",
    "    def split_single_text(text: str) -> list[str]:\n",
    "        if not text:\n",
    "            print(\"Warning: No text provided for splitting.\")\n",
    "            return []\n",
    "\n",
    "        if len(text) <= chunk_size:\n",
    "            return [text]  # If text is smaller than chunk size, return as a single chunk\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        return text_splitter.split_text(text)\n",
    "\n",
    "    # Split each chunk string from the dictionary rows into smaller chunks\n",
    "    all_chunks = [split_single_text(chunk) for chunk in chunk_list]\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "event_chunks = split_events_into_chunks(cat_a_events_dict, chunk_size=2000, chunk_overlap=300)\n",
    "\n",
    "# Print the number of chunks and a sample chunk\n",
    "print(f\"Total chunks: {len(event_chunks)}\\nFirst three chunks:\\n{event_chunks[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Relevant Chunk Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Miniconda\\envs\\master_thesis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Define embeddings \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Incident Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store text chunks into FAISS vector store\n",
    "vectorstore = FAISS.from_texts(report_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for entity: unique accident\n",
      "Searching for entity: accident type\n",
      "Searching for entity: track section\n",
      "Searching for entity: date\n",
      "Searching for entity: time\n",
      "Searching for entity: country\n",
      "Searching for entity: regulatory body\n",
      "\n",
      "Found 4 unique relevant chunks.\n",
      "\n",
      "Most Relevant Chunks Combined:\n",
      "[Page 6]\n",
      "The RAIU make the following safety recommendations:\n",
      "• 202004-01 – IÉ-IM should classify and define RMMEs, Trolleys, LMEs and other\n",
      "commonly used plant or equipment on the railway and ensure appropriate safety\n",
      "procedures are in place for their use. IÉ-IM should also assess the need for any associated\n",
      "training and competency related to these changes and if considered necessary prepare\n",
      "training and competency assessment material;\n",
      "• 202004-02 – IÉ-IM CCE should ensure that, once defined and classified, change\n",
      "management systems are put in place to ensure RMMEs, Trolleys, LMEs, etc are not\n",
      "altered for other uses, without first having been safety validated in line with company\n",
      "processes;\n",
      "• 202004-03 – IÉ-IM should update their Mobile Gang Work Instructions, I-PWY-1490,\n",
      "(Ganger’s Handbook) to ensure that all routine light maintenance activities are included.\n",
      "Systems, e.g., training, should be put in place to ensure that relevant staff can undertake\n",
      "dynamic risk assessments should non-routine activities need to be undertaken that are not\n",
      "described in the Ganger’s Handbook.;\n",
      "• 202004-04 – IÉ-IM clearly define the role of the PWI/APWI and update the relevant\n",
      "documentation accordingly.\n",
      "Railway Accident Investigation Unit v\n",
      "[Page 4]\n",
      "Summary\n",
      "On 11th January 2020, T3 Possessions were granted between the 112 Mile Post (MP) 880\n",
      "yards and the 112 MP 1320 yards (Rosslare Strand to Rosslare Europort) for Iarnród Éireann’s\n",
      "Infrastructure Manager’s (IÉ-IM) Chief Civil Engineer’s (CCE) Department in relation to the\n",
      "Rosslare Coastal Erosion Project.\n",
      "The work being undertaken in the T3 Possessions included the erection of viewing distance\n",
      "marker boards (sometimes referred to as V Boards) on the north and south sides of a newly\n",
      "re-opened level crossing, adjacent to Hayesland IÉ-IM compound. There were four members\n",
      "of CCE staff, working in pairs, erecting the V Boards on the north and south sides of the level\n",
      "crossing. The pair working on the south side included the Person In Charge Of Possession\n",
      "(PICOP)/ Engineering Supervisor (PICOP/ES) and the other pair, working on the north side,\n",
      "included the Acting Permanent Way Inspector (APWI).\n",
      "The APWI decided to transport the V Boards using a piece of wheeled rail-mounted\n",
      "maintenance equipment (RMME) at the level crossing. On arrival at the V Board erection\n",
      "location, it was discovered that the poles to hold the V Boards were too short and needed to\n",
      "be extended; this could be done by a welder working in Hayesland compound. Therefore, the\n",
      "APWI made the decision to put the V Boards back on the RMME and return to the Hayesland\n",
      "compound where the welder could extend the poles to the correct length.\n",
      "On arrival at level crossing (adjacent to Hayesland compound), APWI and another permanent\n",
      "way worker carried one pole each to the welder, leaving the RMME on the track. As APWI\n",
      "was explaining the requirements to the welder his mobile phone rang and he took the call. On\n",
      "completion of the call, the PICOP/ES asked the APWI if the line was clear for the possession\n",
      "to be handed back; the APWI responded that it was clear; intending to remove the RMME.\n",
      "The APWI continued his conversation with the welder, forgetting to remove the RMME.\n",
      "[Page 5]\n",
      "Train A602 struck the RMME as a result of the following causal factors (CaF):\n",
      "• CaF-01 – The APWI used the RMME without authorisation from the PICOP/ES as required\n",
      "under Section A and B of the IÉ Rule Book;\n",
      "• CaF-02 – The APWI did not remove the RMME immediately after removing the V Boards;\n",
      "• CaF-03 – The APWI advised the PICOP/ES that the line was clear when the RMME\n",
      "remained on the line;\n",
      "• CaF-04 – The PICOP/ES did not ensure that the line was clear and safe for trains to pass\n",
      "before giving up the T3 Possession as set out in Sections B and T of the IÉ Rule Book.\n",
      "Contributory factors (CoF) include:\n",
      "• CoF-01 – Proper communication protocols, as set out in Section A of the IÉ Rule Book,\n",
      "between the APWI and the PICOP/ES, were not followed;\n",
      "• CoF-02 – The Ganger’s Handbook does not include any references to maintenance at\n",
      "level crossing or the erection of signage;\n",
      "• CoF-03 – Had the roles of the APWI been supervisory, the APWI may have been more\n",
      "focused on the removal of the RMME rather than the work being carried out.\n",
      "Systemic factors (SF) include:\n",
      "• SF-01 – There is an over-reliance on the Ganger’s Handbook and Site Safety Briefings to\n",
      "address all works that may be carried out by permanent way staff.\n",
      "Although not causal, contributing or systemic, the RAIU make the following additional\n",
      "observation (AO):\n",
      "• AO-01 – The IÉ Rule Book does not clearly classify RMMEs, (Hand) trolleys, Light\n",
      "Maintenance Equipment (LME) or small plant in the IÉ Rule Book or other supporting\n",
      "documentation.\n",
      "Railway Accident Investigation Unit iv\n",
      "completion of the call, the PICOP/ES asked the APWI if the line was clear for the possession\n",
      "to be handed back; the APWI responded that it was clear; intending to remove the RMME.\n",
      "The APWI continued his conversation with the welder, forgetting to remove the RMME.\n",
      "The PICOP/ES phoned the Signalman, without physically checking the line, and handed back\n",
      "the possession, as safe, for the passage of trains.\n",
      "At approximately 10:52 hours (hrs), as the driver of the 08:05 hrs Connolly to Rosslare\n",
      "Europort passenger service (Train A602) approached the level crossing; he saw the RMME\n",
      "on the track and applied the emergency brakes. However, the train collided with the RMME.\n",
      "When the train stopped, the driver informed the relevant staff. The RMME was wedged\n",
      "between the two wheelsets on the leading bogie and required the intervention of permanent\n",
      "way staff to remove the RMME. On removal, the driver inspected the train for damage and\n",
      "after a conversation with the Chief Mechanical Engineer’s (CME) maintenance staff, the train\n",
      "was cleared to continue its journey to Rosslare Europort, twenty-six minutes late.\n",
      "Railway Accident Investigation Unit iii\n"
     ]
    }
   ],
   "source": [
    "# Define entities of interest that you'd like to extract chunks for from the vector store\n",
    "entities_of_interest = [\"unique accident\", \"accident type\", \"track section\", \"date\", \"time\", \"country\", \"regulatory body\"]\n",
    "\n",
    "# Function for extracting most relevant chunks from vector store\n",
    "def find_most_relevant_chunks(entities: list[str], top_k: int) -> str:\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for each entity of interest\n",
    "    using FAISS similarity search and removes duplicates (if same chunk retrieved).\n",
    "    \n",
    "    Args:\n",
    "    - entities (list): List of entity names to query (e.g., [\"date\", \"location\"])\n",
    "    - top_k (int): Number of chunks to retrieve per entity\n",
    "    \n",
    "    Returns:\n",
    "    - unique_relevant_chunks (list): Deduplicated relevant chunks\n",
    "    \"\"\"\n",
    "    retrieved_chunks = set()  # Use a set to avoid duplicate chunks\n",
    "\n",
    "    for entity in entities:\n",
    "        print(f\"Searching for entity: {entity}\")\n",
    "        query = f\"Report details about {entity}.\"\n",
    "        found_chunks = vectorstore.similarity_search(query, k=top_k)\n",
    "\n",
    "        for chunk in found_chunks:\n",
    "            retrieved_chunks.add(chunk.page_content)  # Add chunk if not already present\n",
    "\n",
    "    # Convert set back to a list and join into a single string\n",
    "    unique_relevant_chunks = list(retrieved_chunks)\n",
    "    combined_text = \"\\n\".join(unique_relevant_chunks)\n",
    "\n",
    "    print(f\"\\nFound {len(unique_relevant_chunks)} unique relevant chunks.\")\n",
    "    return combined_text\n",
    "\n",
    "# Find & combine relevant chunks\n",
    "relevant_report_text = find_most_relevant_chunks(entities_of_interest, top_k=3)\n",
    "\n",
    "print(f\"\\nMost Relevant Chunks Combined:\\n{relevant_report_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Accident Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists with event chunks for FAISS processing\n",
    "flat_event_chunks = [chunk for sublist in event_chunks for chunk in sublist]\n",
    "\n",
    "# Store text chunks into FAISS vector store\n",
    "vectorstore = FAISS.from_texts(flat_event_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accident Type: Collisions\n",
      "Accident Type: Other collision type\n",
      "Accident Type: Collision of train with a train/rail vehicle\n",
      "Accident Type: Collision of one or more rail vehicles with another rail vehicle\n",
      "Accident Type: Collision of one or more rail vehicles with obstacle within the clearance gauge\n"
     ]
    }
   ],
   "source": [
    "# Define the accident type to search for\n",
    "accident_type = \"Collision\"\n",
    "\n",
    "# Function for extracting most relevant chunks from vector store\n",
    "def find_most_relevant_chunks(top_k: int) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant accident category chunks from the vector store based on the query.\n",
    "    \"\"\"\n",
    "    query = f\"{accident_type}\"\n",
    "    \n",
    "    # Perform the similarity search\n",
    "    found_chunks = vectorstore.similarity_search(query, k=top_k)\n",
    "    \n",
    "    # Extract the text from each document in the list\n",
    "    found_chunks = [doc.page_content for doc in found_chunks]\n",
    "    \n",
    "    # Join the list of texts into a single string\n",
    "    found_chunks = \"\\n\".join(found_chunks)\n",
    "    \n",
    "    return found_chunks\n",
    "\n",
    "# Find & combine relevant chunks\n",
    "relevant_events_text = find_most_relevant_chunks(top_k=5)\n",
    "\n",
    "print(f\"{relevant_events_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Instantiating Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Instantiating GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key and model name\n",
    "MODEL_GPT = \"gpt-4o-mini\"\n",
    "\n",
    "# Load OpenAI API Key from requirements file\n",
    "with open(\"gpt-personal-key.txt\", \"r\") as file:\n",
    "    OPENAI_API_KEY = file.read().strip()\n",
    "\n",
    "# Instantiate OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating tokens\n",
    "def count_tokens(text: str, model: str = \"gpt-4o\") -> int:\n",
    "    \"\"\"Efficiently counts tokens in a text for a given OpenAI model.\"\"\"\n",
    "    if model not in count_tokens.encoders:\n",
    "        count_tokens.encoders[model] = tiktoken.encoding_for_model(model)\n",
    "    return len(count_tokens.encoders[model].encode(text))\n",
    "\n",
    "count_tokens.encoders = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes for the entities extraction\n",
    "class Property(BaseModel):\n",
    "    \"\"\"A single property consisting of key and value.\"\"\"\n",
    "    key: str = Field(..., description=\"Property key\")\n",
    "    value: str = Field(..., description=\"Property value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    \"\"\"Represents an entity in the railway accident knowledge graph.\"\"\"\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    \"\"\"Represents a relationship between two entities in the graph.\"\"\"\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"A knowledge graph storing railway accident data.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Building the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to build the prompt using the extracted text and other relevant variables\n",
    "def build_gpt_prompt(text):\n",
    "    \"\"\"\n",
    "    Constructs a structured prompt to extract entities and relationships for railway accidents.\n",
    "    \"\"\"\n",
    "    \n",
    "    schema_example = \"\"\"\n",
    "    {\n",
    "        \"nodes\": [\n",
    "            {\"id\": \"Dublin-Cork Accident\", \"type\": \"UniqueAccident\"},\n",
    "            {\"id\": \"Level Crossing Accident involving a train\", \"type\": \"AccidentType\"},\n",
    "            {\"id\": \"105 MP-108 MP\", \"type\": \"TrackSection\"},\n",
    "            {\"id\": \"23/12/2021\", \"type\": \"Date\"},\n",
    "            {\"id\": \"16:32\", \"type\": \"Time\"},\n",
    "            {\"id\": \"Ireland\", \"type\": \"Country\"},\n",
    "            {\"id\": \"European Rail Agency\", \"type\": \"RegulatoryBody\"}\n",
    "        ],\n",
    "        \"rels\": [\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"Ireland\", \"type\": \"occurred_in\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"Collision\", \"type\": \"is_type\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"105 MP-108 MP\", \"type\": \"is_track_section\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"European Rail Agency\", \"type\": \"investigated_by\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"23/12/2021\", \"type\": \"has_date\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"16:32\", \"type\": \"has_time\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "    Analyze the following railway accident report context and extract structured knowledge in JSON format.\n",
    "\n",
    "    Return a JSON object with:\n",
    "    - `nodes`: A list of entities, specifically {entities_of_interest}.\n",
    "    - `rels`: A list of relationships linking entities.\n",
    "\n",
    "    Guidelines:\n",
    "    - Look at the JSON schema example response and follow it closely. \n",
    "    - Ensure that the `source` and `target` nodes in `rels` are the SAME entities from the `nodes` list, and NOT different ones. \n",
    "    - Make sure to map ALL nodes with other important entities, e.g., (node UniqueAccident has_date node Date, node UniqueAccident occurred_at node Country).\n",
    "    - Do NOT map entities like (node Date is_date to node Time) or (node AccidentType is_type to node Country). This is INCORRECT.\n",
    "    - The `type` field in `rels` should be a verb phrase (e.g., \"occurred_in\", \"investigated_by\").\n",
    "    - The `id` field in `nodes` should be the exact text of the entity, not a description or a summary.\n",
    "    - Pay attention to date and type formats (e.g., EU date format, 24-hour time).\n",
    "    - The `UniqueAccident` entity should be a unique identifier for the accident.\n",
    "    - The `AccidentType` entity should be the type of accident. Choose the most relevant one from the list of category events below, and copy it word for word.\n",
    "    - The `TrackSection` entity should be the track section where the accident occurred.\n",
    "    - The `Country` entity should be the country where the accident occurred.\n",
    "    - The `RegulatoryBody` entity should be the regulatory body that investigated the accident.\n",
    "\n",
    "    Schema example:\n",
    "    {schema_example}\n",
    "\n",
    "    Accident type:\n",
    "    {relevant_events_text}\n",
    "\n",
    "    Accident report context:\n",
    "    {text}\n",
    "\n",
    "    JSON:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4 Calling GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to call GPT model with respective prompt\n",
    "def call_gpt(prompt, temperature=1):\n",
    "    \"\"\"\n",
    "    Calls the GPT model with the structured prompt and returns the raw response.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in analyzing railway accident reports. Return output in JSON format only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    response_text = completion.choices[0].message.content.strip()\n",
    "    response_text = re.sub(r'^```json\\n?|```$', '', response_text).strip()\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5 Extracting entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to run the GPT model\n",
    "def extract_entities(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts entities & relationships from a railway accident report using GPT.\n",
    "    If function fails, saves the raw response to a file for review.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build prompt\n",
    "    prompt = build_gpt_prompt(text)\n",
    "\n",
    "    # Call GPT\n",
    "    response_text = call_gpt(prompt)\n",
    "\n",
    "    try:\n",
    "        extracted_graph = json.loads(response_text)  # Ensure valid JSON\n",
    "        return extracted_graph  # Successfully parsed knowledge graph\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON:\", str(e))\n",
    "        print(\"Storing raw response for review...\")\n",
    "\n",
    "        # Save the faulty response for debugging\n",
    "        with open(\"failed_graph_extractions.json\", \"a\") as file:\n",
    "            json.dump({\"input_text\": text[:1000], \"raw_output\": response_text}, file, indent=4)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "        return {}  # Return empty dictionary in case of failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.6 Token approval & execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost: $0.00033\n",
      "Token count for prompt: 2197\n"
     ]
    }
   ],
   "source": [
    "# Define token limit for function execution\n",
    "token_limit = 4096\n",
    "\n",
    "# Build the prompt and count tokens\n",
    "prompt = build_gpt_prompt(relevant_report_text)\n",
    "token_count = count_tokens(prompt)\n",
    "estimated_cost = token_count * 0.00000015  # Approximate OpenAI pricing\n",
    "\n",
    "# Check token limit\n",
    "print(f\"Estimated cost: ${estimated_cost:.5f}\")\n",
    "if token_count > token_limit:\n",
    "    print(f\"Token count is too high: {token_count}\\nPlease reduce the chunk size or refine the prompt.\")\n",
    "else:\n",
    "    print(f\"Token count for prompt: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to GPT...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'id': 'Train A602 Collision', 'type': 'UniqueAccident'},\n",
       "  {'id': 'Collision of one or more rail vehicles with obstacle within the clearance gauge',\n",
       "   'type': 'AccidentType'},\n",
       "  {'id': '112 MP 880 yards to 112 MP 1320 yards', 'type': 'TrackSection'},\n",
       "  {'id': '11/01/2020', 'type': 'Date'},\n",
       "  {'id': '10:52', 'type': 'Time'},\n",
       "  {'id': 'Ireland', 'type': 'Country'},\n",
       "  {'id': 'Railway Accident Investigation Unit', 'type': 'RegulatoryBody'}],\n",
       " 'rels': [{'source': 'Train A602 Collision',\n",
       "   'target': 'Ireland',\n",
       "   'type': 'occurred_in'},\n",
       "  {'source': 'Train A602 Collision',\n",
       "   'target': 'Collision of one or more rail vehicles with obstacle within the clearance gauge',\n",
       "   'type': 'is_type'},\n",
       "  {'source': 'Train A602 Collision',\n",
       "   'target': '112 MP 880 yards to 112 MP 1320 yards',\n",
       "   'type': 'is_track_section'},\n",
       "  {'source': 'Train A602 Collision',\n",
       "   'target': 'Railway Accident Investigation Unit',\n",
       "   'type': 'investigated_by'},\n",
       "  {'source': 'Train A602 Collision',\n",
       "   'target': '11/01/2020',\n",
       "   'type': 'has_date'},\n",
       "  {'source': 'Train A602 Collision', 'target': '10:52', 'type': 'has_time'}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm execution\n",
    "proceed = input(\"Do you want to proceed with information extraction? (yes/no): \").strip().lower()\n",
    "if proceed != \"yes\":\n",
    "    print(\"Extraction aborted by user.\")\n",
    "else:\n",
    "    print(\"Sending request to GPT...\")\n",
    "    gpt_response = extract_entities(relevant_report_text)\n",
    "    response_dict = {\"model\": MODEL_GPT, \"response\": gpt_response}\n",
    "    \n",
    "gpt_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Instantiating Local Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Building the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to build the prompt for local models\n",
    "def build_local_prompt(text):\n",
    "    \"\"\"\n",
    "    Constructs a structured prompt to extract entities and relationships for railway accidents.\n",
    "    \"\"\"\n",
    "    \n",
    "    schema_example = \"\"\"\n",
    "    {\n",
    "        \"nodes\": [\n",
    "            {\"id\": \"Dublin-Cork Accident\", \"type\": \"UniqueAccident\"},\n",
    "            {\"id\": \"Train Derailment\", \"type\": \"AccidentType\"},\n",
    "            {\"id\": \"23/12/2021\", \"type\": \"Date\"},\n",
    "            {\"id\": \"16:32\", \"type\": \"Time\"},\n",
    "            {\"id\": \"Ireland\", \"type\": \"Country\"},\n",
    "            {\"id\": \"European Rail Agency\", \"type\": \"RegulatoryBody\"}\n",
    "        ],\n",
    "        \"rels\": [\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"Ireland\", \"type\": \"occurred_in\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"Collision\", \"type\": \"is_type\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"European Rail Agency\", \"type\": \"investigated_by\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"23/12/2021\", \"type\": \"has_date\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"16:32\", \"type\": \"has_time\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "    Extract structured entities and output ONLY a JSON object from this railway accident report. Do not provide a summary or comment on the incident.\n",
    "\n",
    "    Return the JSON object with:\n",
    "    - `nodes`: A list of entities, specifically {entities_of_interest}.\n",
    "    - `rels`: A list of relationships linking entities.\n",
    "\n",
    "    Schema example:\n",
    "    {schema_example}\n",
    "\n",
    "    Accident report context:\n",
    "    {text}\n",
    "\n",
    "    JSON:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Defining the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API URL\n",
    "url = \"http://llama-max-ollama.ai.wu.ac.at/api/generate\"\n",
    "\n",
    "# Define prompt\n",
    "prompt = build_local_prompt(relevant_report_text)\n",
    "\n",
    "# Specify local model\n",
    "MODEL_LOCAL = \"llama3.1:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Querying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload and query the local model\n",
    "payload = {\n",
    "    \"model\": f\"{MODEL_LOCAL}\",  # Ensure correct model name\n",
    "    \"prompt\": f\"{prompt}\",\n",
    "    \"stream\": False  # If 'raw' is unnecessary, remove it\n",
    "}\n",
    "\n",
    "# Set headers\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Send POST request\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# Handle response\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        data = response.json()  # Parse response JSON\n",
    "        if \"response\" in data:\n",
    "            local_response = textwrap.fill(data[\"response\"], width=80)\n",
    "            print(\"Generated Summary:\\n\")\n",
    "            print(local_response)\n",
    "        else:\n",
    "            print(\"No 'response' key found in the JSON.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON response: {response.text}\")\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 Extracting JSON from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    # List of regex patterns to try.\n",
    "    patterns = [\n",
    "        # Pattern for JSON wrapped in a markdown code block:\n",
    "        r'```json\\s*([\\s\\S]*?)\\s*```',\n",
    "        # Fallback pattern: JSON object starting with '{' and ending with '}'\n",
    "        r'({[\\s\\S]*})'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "            # Optionally, remove unwanted control characters.\n",
    "            json_str = re.sub(r'[\\x00-\\x1F]+', '', json_str)\n",
    "            try:\n",
    "                return json.loads(json_str)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"JSON decode error:\", e)\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "local_response = extract_json(local_response)\n",
    "response_dict = {\"model\": MODEL_LOCAL, \"response\": local_response}\n",
    "local_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cross-model and Iteration Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Overview of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added IE-6218-200111 Collision RRME Rosslare.pdf - Iteration 2 to results!\n"
     ]
    }
   ],
   "source": [
    "# Define CSV storage file\n",
    "CSV_FILE = \"pdf_processing_results.csv\"\n",
    "\n",
    "# Define function to append the JSON output of response_json function to a DataFrame\n",
    "def append_pdf_json_result(pdf_name: str, response_json: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Appends the JSON output of response_json function to a DataFrame.\n",
    "    \n",
    "    - If the PDF has been processed before, it appends a **new row** instead of a new column.\n",
    "    - Prevents duplicate JSON entries for the same iteration.\n",
    "    - Ensures data is **stored in rows**, making querying and analysis easier.\n",
    "\n",
    "    Args:\n",
    "        pdf_name (str): Name of the processed PDF file.\n",
    "        response_json (dict): JSON response from the knowledge extraction process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with the new result.\n",
    "    \"\"\"\n",
    "    # Convert JSON response to a formatted string for easy comparison\n",
    "    json_output = json.dumps(response_json, indent=2)\n",
    "\n",
    "    # Load existing results if the CSV exists\n",
    "    path = \"./data/\" + CSV_FILE\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path, dtype={\"iteration_number\": int})\n",
    "    else:\n",
    "        # Create an empty DataFrame with the correct schema\n",
    "        df = pd.DataFrame(columns=[\"pdf_name\", \"model_type\", \"iteration_number\", \"json_output\"])\n",
    "\n",
    "    model_type = MODEL_GPT if response_dict.get(\"model\") == MODEL_GPT else MODEL_LOCAL\n",
    "\n",
    "    # Filter for the current PDF's past records\n",
    "    pdf_history = df[df[\"pdf_name\"] == pdf_name]\n",
    "\n",
    "    # Check for duplicates: If this JSON output already exists for the same PDF, skip re-adding\n",
    "    if not pdf_history.empty and json_output in pdf_history[\"json_output\"].values:\n",
    "        print(f\"No changes detected in JSON for {pdf_name}, skipping new entry.\")\n",
    "        return df  # Exit early if it's a duplicate\n",
    "\n",
    "    # Determine new iteration number\n",
    "    iteration_number = pdf_history[\"iteration_number\"].max() + 1 if not pdf_history.empty else 1\n",
    "\n",
    "    # Append new result\n",
    "    new_entry = pd.DataFrame({\"pdf_name\": [pdf_name], \"model_type\": [model_type], \"iteration_number\": [iteration_number], \"json_output\": [json_output]})\n",
    "    df = pd.concat([df, new_entry], ignore_index=True)\n",
    "\n",
    "    # Save back to CSV in **append mode** to avoid full file reads/writes\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "    print(f\"Successfully added {pdf_name} - Iteration {iteration_number} to results!\")\n",
    "    return df\n",
    "\n",
    "# Example execution\n",
    "results_df = append_pdf_json_result(pdf_name, response_dict[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pdf_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "iteration_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "json_output",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e75c6720-1a43-48cd-8797-e41a85a994c7",
       "rows": [
        [
         "0",
         "IE-10375 - 210827 Collision with track equipment.pdf",
         "gpt-4o-mini",
         "1",
         "{\n  \"nodes\": [\n    {\n      \"id\": \"Train J283 Accident\",\n      \"type\": \"UniqueAccident\"\n    },\n    {\n      \"id\": \"Train Striking an Object\",\n      \"type\": \"AccidentType\"\n    },\n    {\n      \"id\": \"20/11/2022\",\n      \"type\": \"Date\"\n    },\n    {\n      \"id\": \"00:16\",\n      \"type\": \"Time\"\n    },\n    {\n      \"id\": \"Ireland\",\n      \"type\": \"Country\"\n    },\n    {\n      \"id\": \"Iarnr\\u00f3d \\u00c9ireann\",\n      \"type\": \"RegulatoryBody\"\n    }\n  ],\n  \"rels\": [\n    {\n      \"source\": \"Train J283 Accident\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_in\"\n    },\n    {\n      \"source\": \"Train J283 Accident\",\n      \"target\": \"Train Striking an Object\",\n      \"type\": \"is_type\"\n    },\n    {\n      \"source\": \"Train J283 Accident\",\n      \"target\": \"Iarnr\\u00f3d \\u00c9ireann\",\n      \"type\": \"investigated_by\"\n    },\n    {\n      \"source\": \"Train J283 Accident\",\n      \"target\": \"20/11/2022\",\n      \"type\": \"has_date\"\n    },\n    {\n      \"source\": \"Train J283 Accident\",\n      \"target\": \"00:16\",\n      \"type\": \"has_time\"\n    }\n  ]\n}"
        ],
        [
         "1",
         "IE-10397 - 211207 Clontarf.pdf",
         "gpt-4o-mini",
         "1",
         "{\n  \"nodes\": [\n    {\n      \"id\": \"Incident involving two consecutive signals passed at danger\",\n      \"type\": \"UniqueAccident\"\n    },\n    {\n      \"id\": \"Incident \\u2013 Train Operations & Management\",\n      \"type\": \"AccidentType\"\n    },\n    {\n      \"id\": \"07/12/2021\",\n      \"type\": \"Date\"\n    },\n    {\n      \"id\": \"00:00\",\n      \"type\": \"Time\"\n    },\n    {\n      \"id\": \"Ireland\",\n      \"type\": \"Country\"\n    },\n    {\n      \"id\": \"Commission for Railway Regulation\",\n      \"type\": \"RegulatoryBody\"\n    }\n  ],\n  \"rels\": [\n    {\n      \"source\": \"Incident involving two consecutive signals passed at danger\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_in\"\n    },\n    {\n      \"source\": \"Incident involving two consecutive signals passed at danger\",\n      \"target\": \"Incident \\u2013 Train Operations & Management\",\n      \"type\": \"is_type\"\n    },\n    {\n      \"source\": \"Incident involving two consecutive signals passed at danger\",\n      \"target\": \"Commission for Railway Regulation\",\n      \"type\": \"investigated_by\"\n    },\n    {\n      \"source\": \"Incident involving two consecutive signals passed at danger\",\n      \"target\": \"07/12/2021\",\n      \"type\": \"has_date\"\n    },\n    {\n      \"source\": \"Incident involving two consecutive signals passed at danger\",\n      \"target\": \"00:00\",\n      \"type\": \"has_time\"\n    }\n  ]\n}"
        ],
        [
         "2",
         "IE-6218-200111 Collision RRME Rosslare.pdf",
         "gpt-4o-mini",
         "1",
         "{\n  \"nodes\": [\n    {\n      \"id\": \"Train A602 Collision with RMME\",\n      \"type\": \"UniqueAccident\"\n    },\n    {\n      \"id\": \"Collision\",\n      \"type\": \"AccidentType\"\n    },\n    {\n      \"id\": \"112 MP 880 yards to 112 MP 1320 yards\",\n      \"type\": \"TrackSection\"\n    },\n    {\n      \"id\": \"11/01/2020\",\n      \"type\": \"Date\"\n    },\n    {\n      \"id\": \"10:52\",\n      \"type\": \"Time\"\n    },\n    {\n      \"id\": \"Ireland\",\n      \"type\": \"Country\"\n    },\n    {\n      \"id\": \"Railway Accident Investigation Unit\",\n      \"type\": \"RegulatoryBody\"\n    }\n  ],\n  \"rels\": [\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_in\"\n    },\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"Collision\",\n      \"type\": \"is_type\"\n    },\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"112 MP 880 yards to 112 MP 1320 yards\",\n      \"type\": \"is_track_section\"\n    },\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"Railway Accident Investigation Unit\",\n      \"type\": \"investigated_by\"\n    },\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"11/01/2020\",\n      \"type\": \"has_date\"\n    },\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"10:52\",\n      \"type\": \"has_time\"\n    }\n  ]\n}"
        ],
        [
         "3",
         "IE-6218-200111 Collision RRME Rosslare.pdf",
         "gpt-4o-mini",
         "2",
         "{\n  \"nodes\": [\n    {\n      \"id\": \"Train A602 Collision\",\n      \"type\": \"UniqueAccident\"\n    },\n    {\n      \"id\": \"Collision of one or more rail vehicles with obstacle within the clearance gauge\",\n      \"type\": \"AccidentType\"\n    },\n    {\n      \"id\": \"112 MP 880 yards to 112 MP 1320 yards\",\n      \"type\": \"TrackSection\"\n    },\n    {\n      \"id\": \"11/01/2020\",\n      \"type\": \"Date\"\n    },\n    {\n      \"id\": \"10:52\",\n      \"type\": \"Time\"\n    },\n    {\n      \"id\": \"Ireland\",\n      \"type\": \"Country\"\n    },\n    {\n      \"id\": \"Railway Accident Investigation Unit\",\n      \"type\": \"RegulatoryBody\"\n    }\n  ],\n  \"rels\": [\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_in\"\n    },\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"Collision of one or more rail vehicles with obstacle within the clearance gauge\",\n      \"type\": \"is_type\"\n    },\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"112 MP 880 yards to 112 MP 1320 yards\",\n      \"type\": \"is_track_section\"\n    },\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"Railway Accident Investigation Unit\",\n      \"type\": \"investigated_by\"\n    },\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"11/01/2020\",\n      \"type\": \"has_date\"\n    },\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"10:52\",\n      \"type\": \"has_time\"\n    }\n  ]\n}"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>iteration_number</th>\n",
       "      <th>json_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IE-10375 - 210827 Collision with track equipme...</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>1</td>\n",
       "      <td>{\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train J28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IE-10397 - 211207 Clontarf.pdf</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>1</td>\n",
       "      <td>{\\n  \"nodes\": [\\n    {\\n      \"id\": \"Incident ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IE-6218-200111 Collision RRME Rosslare.pdf</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>1</td>\n",
       "      <td>{\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train A60...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IE-6218-200111 Collision RRME Rosslare.pdf</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>2</td>\n",
       "      <td>{\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train A60...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pdf_name   model_type  \\\n",
       "0  IE-10375 - 210827 Collision with track equipme...  gpt-4o-mini   \n",
       "1                     IE-10397 - 211207 Clontarf.pdf  gpt-4o-mini   \n",
       "2         IE-6218-200111 Collision RRME Rosslare.pdf  gpt-4o-mini   \n",
       "3         IE-6218-200111 Collision RRME Rosslare.pdf  gpt-4o-mini   \n",
       "\n",
       "   iteration_number                                        json_output  \n",
       "0                 1  {\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train J28...  \n",
       "1                 1  {\\n  \"nodes\": [\\n    {\\n      \"id\": \"Incident ...  \n",
       "2                 1  {\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train A60...  \n",
       "3                 2  {\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train A60...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Mapping entities to graph nods and rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for mapping extracted entities to graph nodes and relationships\n",
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Converts properties to a dictionary for graph storage.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "        return properties\n",
    "    for p in props:\n",
    "        properties[p[\"key\"]] = p[\"value\"]\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Maps extracted entities to graph nodes.\"\"\"\n",
    "    properties = {\"name\": node.id}\n",
    "    return BaseNode(\n",
    "        id=node.id,\n",
    "        type=node.type.capitalize(),\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Maps extracted relationships to graph edges.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Neo4j Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Instantiating Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j successfully.\n"
     ]
    }
   ],
   "source": [
    "# Neo4j Connection Setup\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\"\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "try:\n",
    "    # Test the connection\n",
    "    with driver.session() as session:\n",
    "        session.run(\"RETURN 1\")\n",
    "    print(\"Connected to Neo4j successfully.\")\n",
    "except AuthError as e:\n",
    "    print(\"Authentication failed. Check your credentials:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j database cleared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Clear database\n",
    "def clear_neo4j_database():\n",
    "    \"\"\"Delete all nodes and relationships in the Neo4j database.\"\"\"\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    print(\"Neo4j database cleared successfully.\")\n",
    "\n",
    "# Run the function to clear the database\n",
    "clear_neo4j_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pdf_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "iteration_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "json_output",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "18359aee-90f9-4de4-b5f3-4b51b3d99c26",
       "rows": [
        [
         "0",
         "IE-10375 - 210827 Collision with track equipment.pdf",
         "gpt-4o-mini",
         "1",
         "{\n  \"nodes\": [\n    {\n      \"id\": \"Train J283 Accident\",\n      \"type\": \"UniqueAccident\"\n    },\n    {\n      \"id\": \"Train Striking an Object\",\n      \"type\": \"AccidentType\"\n    },\n    {\n      \"id\": \"20/11/2022\",\n      \"type\": \"Date\"\n    },\n    {\n      \"id\": \"00:16\",\n      \"type\": \"Time\"\n    },\n    {\n      \"id\": \"Ireland\",\n      \"type\": \"Country\"\n    },\n    {\n      \"id\": \"Iarnr\\u00f3d \\u00c9ireann\",\n      \"type\": \"RegulatoryBody\"\n    }\n  ],\n  \"rels\": [\n    {\n      \"source\": \"Train J283 Accident\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_in\"\n    },\n    {\n      \"source\": \"Train J283 Accident\",\n      \"target\": \"Train Striking an Object\",\n      \"type\": \"is_type\"\n    },\n    {\n      \"source\": \"Train J283 Accident\",\n      \"target\": \"Iarnr\\u00f3d \\u00c9ireann\",\n      \"type\": \"investigated_by\"\n    },\n    {\n      \"source\": \"Train J283 Accident\",\n      \"target\": \"20/11/2022\",\n      \"type\": \"has_date\"\n    },\n    {\n      \"source\": \"Train J283 Accident\",\n      \"target\": \"00:16\",\n      \"type\": \"has_time\"\n    }\n  ]\n}"
        ],
        [
         "1",
         "IE-10397 - 211207 Clontarf.pdf",
         "gpt-4o-mini",
         "1",
         "{\n  \"nodes\": [\n    {\n      \"id\": \"Incident involving two consecutive signals passed at danger\",\n      \"type\": \"UniqueAccident\"\n    },\n    {\n      \"id\": \"Incident \\u2013 Train Operations & Management\",\n      \"type\": \"AccidentType\"\n    },\n    {\n      \"id\": \"07/12/2021\",\n      \"type\": \"Date\"\n    },\n    {\n      \"id\": \"00:00\",\n      \"type\": \"Time\"\n    },\n    {\n      \"id\": \"Ireland\",\n      \"type\": \"Country\"\n    },\n    {\n      \"id\": \"Commission for Railway Regulation\",\n      \"type\": \"RegulatoryBody\"\n    }\n  ],\n  \"rels\": [\n    {\n      \"source\": \"Incident involving two consecutive signals passed at danger\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_in\"\n    },\n    {\n      \"source\": \"Incident involving two consecutive signals passed at danger\",\n      \"target\": \"Incident \\u2013 Train Operations & Management\",\n      \"type\": \"is_type\"\n    },\n    {\n      \"source\": \"Incident involving two consecutive signals passed at danger\",\n      \"target\": \"Commission for Railway Regulation\",\n      \"type\": \"investigated_by\"\n    },\n    {\n      \"source\": \"Incident involving two consecutive signals passed at danger\",\n      \"target\": \"07/12/2021\",\n      \"type\": \"has_date\"\n    },\n    {\n      \"source\": \"Incident involving two consecutive signals passed at danger\",\n      \"target\": \"00:00\",\n      \"type\": \"has_time\"\n    }\n  ]\n}"
        ],
        [
         "2",
         "IE-6218-200111 Collision RRME Rosslare.pdf",
         "gpt-4o-mini",
         "1",
         "{\n  \"nodes\": [\n    {\n      \"id\": \"Train A602 Collision with RMME\",\n      \"type\": \"UniqueAccident\"\n    },\n    {\n      \"id\": \"Collision\",\n      \"type\": \"AccidentType\"\n    },\n    {\n      \"id\": \"112 MP 880 yards to 112 MP 1320 yards\",\n      \"type\": \"TrackSection\"\n    },\n    {\n      \"id\": \"11/01/2020\",\n      \"type\": \"Date\"\n    },\n    {\n      \"id\": \"10:52\",\n      \"type\": \"Time\"\n    },\n    {\n      \"id\": \"Ireland\",\n      \"type\": \"Country\"\n    },\n    {\n      \"id\": \"Railway Accident Investigation Unit\",\n      \"type\": \"RegulatoryBody\"\n    }\n  ],\n  \"rels\": [\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_in\"\n    },\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"Collision\",\n      \"type\": \"is_type\"\n    },\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"112 MP 880 yards to 112 MP 1320 yards\",\n      \"type\": \"is_track_section\"\n    },\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"Railway Accident Investigation Unit\",\n      \"type\": \"investigated_by\"\n    },\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"11/01/2020\",\n      \"type\": \"has_date\"\n    },\n    {\n      \"source\": \"Train A602 Collision with RMME\",\n      \"target\": \"10:52\",\n      \"type\": \"has_time\"\n    }\n  ]\n}"
        ],
        [
         "3",
         "IE-6218-200111 Collision RRME Rosslare.pdf",
         "gpt-4o-mini",
         "2",
         "{\n  \"nodes\": [\n    {\n      \"id\": \"Train A602 Collision\",\n      \"type\": \"UniqueAccident\"\n    },\n    {\n      \"id\": \"Collision of one or more rail vehicles with obstacle within the clearance gauge\",\n      \"type\": \"AccidentType\"\n    },\n    {\n      \"id\": \"112 MP 880 yards to 112 MP 1320 yards\",\n      \"type\": \"TrackSection\"\n    },\n    {\n      \"id\": \"11/01/2020\",\n      \"type\": \"Date\"\n    },\n    {\n      \"id\": \"10:52\",\n      \"type\": \"Time\"\n    },\n    {\n      \"id\": \"Ireland\",\n      \"type\": \"Country\"\n    },\n    {\n      \"id\": \"Railway Accident Investigation Unit\",\n      \"type\": \"RegulatoryBody\"\n    }\n  ],\n  \"rels\": [\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_in\"\n    },\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"Collision of one or more rail vehicles with obstacle within the clearance gauge\",\n      \"type\": \"is_type\"\n    },\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"112 MP 880 yards to 112 MP 1320 yards\",\n      \"type\": \"is_track_section\"\n    },\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"Railway Accident Investigation Unit\",\n      \"type\": \"investigated_by\"\n    },\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"11/01/2020\",\n      \"type\": \"has_date\"\n    },\n    {\n      \"source\": \"Train A602 Collision\",\n      \"target\": \"10:52\",\n      \"type\": \"has_time\"\n    }\n  ]\n}"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>iteration_number</th>\n",
       "      <th>json_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IE-10375 - 210827 Collision with track equipme...</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>1</td>\n",
       "      <td>{\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train J28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IE-10397 - 211207 Clontarf.pdf</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>1</td>\n",
       "      <td>{\\n  \"nodes\": [\\n    {\\n      \"id\": \"Incident ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IE-6218-200111 Collision RRME Rosslare.pdf</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>1</td>\n",
       "      <td>{\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train A60...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IE-6218-200111 Collision RRME Rosslare.pdf</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>2</td>\n",
       "      <td>{\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train A60...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pdf_name   model_type  \\\n",
       "0  IE-10375 - 210827 Collision with track equipme...  gpt-4o-mini   \n",
       "1                     IE-10397 - 211207 Clontarf.pdf  gpt-4o-mini   \n",
       "2         IE-6218-200111 Collision RRME Rosslare.pdf  gpt-4o-mini   \n",
       "3         IE-6218-200111 Collision RRME Rosslare.pdf  gpt-4o-mini   \n",
       "\n",
       "   iteration_number                                        json_output  \n",
       "0                 1  {\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train J28...  \n",
       "1                 1  {\\n  \"nodes\": [\\n    {\\n      \"id\": \"Incident ...  \n",
       "2                 1  {\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train A60...  \n",
       "3                 2  {\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train A60...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Extracting the JSON from DF row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"id\": \"Train A602 Collision\",\n",
      "      \"type\": \"UniqueAccident\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Collision of one or more rail vehicles with obstacle within the clearance gauge\",\n",
      "      \"type\": \"AccidentType\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"112 MP 880 yards to 112 MP 1320 yards\",\n",
      "      \"type\": \"TrackSection\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"11/01/2020\",\n",
      "      \"type\": \"Date\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"10:52\",\n",
      "      \"type\": \"Time\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Ireland\",\n",
      "      \"type\": \"Country\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Railway Accident Investigation Unit\",\n",
      "      \"type\": \"RegulatoryBody\"\n",
      "    }\n",
      "  ],\n",
      "  \"rels\": [\n",
      "    {\n",
      "      \"source\": \"Train A602 Collision\",\n",
      "      \"target\": \"Ireland\",\n",
      "      \"type\": \"occurred_in\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Train A602 Collision\",\n",
      "      \"target\": \"Collision of one or more rail vehicles with obstacle within the clearance gauge\",\n",
      "      \"type\": \"is_type\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Train A602 Collision\",\n",
      "      \"target\": \"112 MP 880 yards to 112 MP 1320 yards\",\n",
      "      \"type\": \"is_track_section\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Train A602 Collision\",\n",
      "      \"target\": \"Railway Accident Investigation Unit\",\n",
      "      \"type\": \"investigated_by\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Train A602 Collision\",\n",
      "      \"target\": \"11/01/2020\",\n",
      "      \"type\": \"has_date\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Train A602 Collision\",\n",
      "      \"target\": \"10:52\",\n",
      "      \"type\": \"has_time\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def get_json_output(df, pdf_name, iteration_number):\n",
    "    \"\"\"\n",
    "    Gets the 'json_output' for the given pdf_name and iteration_number.\n",
    "    Returns an empty dict if there's no match.\n",
    "    \"\"\"\n",
    "    subset = df[\n",
    "        (df[\"pdf_name\"] == pdf_name) &\n",
    "        (df[\"iteration_number\"] == iteration_number)\n",
    "    ]\n",
    "\n",
    "    if subset.empty:\n",
    "        print(\"No match found.\")\n",
    "        return {}\n",
    "\n",
    "    json_str = subset.iloc[0][\"json_output\"]\n",
    "    return json.loads(json_str)\n",
    "\n",
    "# Choose the JSON to convert to graph\n",
    "pdf_of_choice = \"IE-6218-200111 Collision RRME Rosslare.pdf\"\n",
    "json_to_convert = get_json_output(results_df, pdf_of_choice, 2)\n",
    "print(json.dumps(json_to_convert, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Converting the JSON to graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_graph(json_to_convert, source_text):\n",
    "    \"\"\"\n",
    "    Converts extracted JSON into a graph-compatible format with correct entity types.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_node_type(json_data, node_id):\n",
    "        \"\"\"\n",
    "        Helper function to retrieve the correct node type from JSON.\n",
    "        \"\"\"\n",
    "        for node in json_data[\"nodes\"]:\n",
    "            if node[\"id\"] == node_id:\n",
    "                return node[\"type\"]\n",
    "        return \"Unknown\"  # Fallback if type is missing\n",
    "\n",
    "    if not json_to_convert:\n",
    "        print(\"No valid data to convert to a graph.\")\n",
    "        return None\n",
    "\n",
    "    # Convert Nodes\n",
    "    graph_nodes = [map_to_base_node(Node(id=node[\"id\"], type=node[\"type\"])) for node in json_to_convert[\"nodes\"]]\n",
    "\n",
    "    # Convert Relationships (Ensure correct types)\n",
    "    graph_rels = []\n",
    "    for rel in json_to_convert[\"rels\"]:\n",
    "        source_node = Node(id=rel[\"source\"], type=get_node_type(json_to_convert, rel[\"source\"]))\n",
    "        target_node = Node(id=rel[\"target\"], type=get_node_type(json_to_convert, rel[\"target\"]))\n",
    "        graph_rels.append(map_to_base_relationship(Relationship(source=source_node, target=target_node, type=rel[\"type\"])))\n",
    "\n",
    "    return GraphDocument(nodes=graph_nodes, relationships=graph_rels, source=Document(page_content=source_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Storing the graph in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_neo4j(graph_document):\n",
    "    \"\"\"\n",
    "    Stores extracted knowledge graph into Neo4j with dynamic labels.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Store nodes with dynamic labels\n",
    "        for node in graph_document.nodes:\n",
    "            session.run(f\"\"\"\n",
    "                MERGE (n:{node.type} {{id: $id}})\n",
    "                ON CREATE SET n.name = $name\n",
    "            \"\"\", id=node.id, name=node.id)\n",
    "\n",
    "        # Store relationships\n",
    "        for rel in graph_document.relationships:\n",
    "            session.run(f\"\"\"\n",
    "                MATCH (s {{id: $source}})\n",
    "                MATCH (t {{id: $target}})\n",
    "                MERGE (s)-[r:{rel.type.upper()}]->(t)\n",
    "            \"\"\", source=rel.source.id, target=rel.target.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_railway_accident_report(json_to_convert):\n",
    "    \"\"\"\n",
    "    Converts the JSON to a graph format and stores it in Neo4j.\n",
    "    \"\"\"\n",
    "    print(\"Converting JSON to graph format...\")\n",
    "    graph_document = convert_json_to_graph(json_to_convert, relevant_report_text)\n",
    "\n",
    "    if graph_document:\n",
    "        print(\"Graph structure created! Storing in Neo4j...\")\n",
    "        store_in_neo4j(graph_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting JSON to graph format...\n",
      "Graph structure created! Storing in Neo4j...\n",
      "Data stored in Neo4j successfully.\n"
     ]
    }
   ],
   "source": [
    "# Store extracted entities into Neo4j\n",
    "try:\n",
    "    db_result = process_railway_accident_report(json_to_convert)\n",
    "    print(\"Data stored in Neo4j successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to store data in Neo4j:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Neo4j connection\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Comparison against ERAIL DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Loading ERAIL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ERAIL DB\n",
    "erail_db = pd.read_excel(\"erail database.xlsx\")\n",
    "erail_db.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime & adjust format\n",
    "erail_db[\"Date of occurrence\"] = erail_db[\"Date of occurrence\"].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# Convert time column to datetime & adjust format\n",
    "erail_db[\"Time of occurrence\"] = pd.to_datetime(erail_db[\"Time of occurrence\"], errors='coerce')\n",
    "erail_db[\"Time of occurrence\"] = erail_db[\"Time of occurrence\"].dt.strftime(\"%H:%M\")\n",
    "\n",
    "erail_db.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Merging data for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the results DataFrame\n",
    "comparison_df = results_df[[\"pdf_name\", \"json_output\"]].copy()\n",
    "\n",
    "# Extract ID from the PDF name\n",
    "comparison_df[\"ERAIL Occurrence\"] = comparison_df[\"pdf_name\"].str.extract(r'(IE-\\d+)')\n",
    "\n",
    "# Sample DataFrame (assuming json_output column contains dictionaries in string format)\n",
    "comparison_df['json_output'] = comparison_df['json_output'].apply(json.loads)  # Convert JSON string to dictionary\n",
    "\n",
    "# Function to extract node data\n",
    "def extract_nodes(json_data):\n",
    "    node_dict = {}\n",
    "    for node in json_data.get(\"nodes\", []):\n",
    "        node_dict[f\"gpt_{node['type']}\"] = node[\"id\"]  # Store ID based on type\n",
    "    return pd.Series(node_dict)  # Convert dictionary to Series for easier DataFrame merging\n",
    "\n",
    "# Apply the function to extract node data\n",
    "nodes_df = comparison_df['json_output'].apply(extract_nodes)\n",
    "\n",
    "# Merge extracted data into original DataFrame\n",
    "comparison_df = pd.concat([comparison_df, nodes_df], axis=1)\n",
    "\n",
    "# Drop the original json_output column if no longer needed\n",
    "comparison_df.drop(columns=[\"json_output\"], inplace=True)\n",
    "\n",
    "# View comparison DataFrame\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the comparison DataFrame with the ERAIL database\n",
    "merged_df = comparison_df.merge(erail_db, on=\"ERAIL Occurrence\", how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GPT extracted date with the ERAIL database (source of truth)\n",
    "merged_df[\"Date Match\"] = np.where(merged_df[\"gpt_Date\"] == merged_df[\"Date of occurrence\"], \"Match\", \"Mismatch\")\n",
    "merged_df[[\"ERAIL Occurrence\", \"gpt_Date\", \"Date of occurrence\", \"Date Match\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GPT extracted time with the ERAIL database (source of truth)\n",
    "merged_df[\"Time Match\"] = np.where(merged_df[\"gpt_Time\"] == merged_df[\"Time of occurrence\"], \"Match\", \"Mismatch\")\n",
    "merged_df[[\"ERAIL Occurrence\", \"gpt_Time\", \"Time of occurrence\", \"Time Match\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GPT extracted country with the ERAIL database (source of truth)\n",
    "merged_df[\"Country Match\"] = np.where(merged_df[\"gpt_Country\"] == merged_df[\"Country\"], \"Match\", \"Mismatch\")\n",
    "merged_df[[\"ERAIL Occurrence\", \"gpt_Country\", \"Country\", \"Country Match\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
