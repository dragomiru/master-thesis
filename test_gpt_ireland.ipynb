{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# PDFs\n",
    "import pdfplumber\n",
    "\n",
    "# LLMs\n",
    "import faiss\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.graphs.graph_document import (\n",
    "    GraphDocument,\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from openai import OpenAI\n",
    "import tiktoken  # If used\n",
    "\n",
    "# Neo4j\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j.exceptions import AuthError\n",
    "\n",
    "# Typing & Validation\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file name\n",
    "pdf_name = \"IE-10397 - 211207 Clontarf.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PATTERN = re.compile(r\"^summary\", re.IGNORECASE)\n",
    "CONTENTS_PATTERN = re.compile(r\"^contents\", re.IGNORECASE)\n",
    "\n",
    "# Define function to extract the summary section from Ireland reports\n",
    "def extract_summary_section(pdf_path, header_lines=1):\n",
    "    \"\"\"\n",
    "    Extracts the text from the pages between 'Summary' and 'Contents', as well as the header (which is usually repeated).\n",
    "    Function adapted only for Ireland report incident PDFs due to consistent structure.\n",
    "    \"\"\"\n",
    "    summary_text = \"\"\n",
    "    capturing = False  # Flag to start capturing text\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text_lines = page_text.split(\"\\n\")\n",
    "                text_without_header = text_lines[header_lines:]  # Remove header lines\n",
    "                \n",
    "                # Ensure there's enough content after the header\n",
    "                if text_without_header:\n",
    "                    first_significant_line = text_without_header[0].strip().lower()\n",
    "                    \n",
    "                    # Start capturing if 'Summary' is found\n",
    "                    if SUMMARY_PATTERN.match(first_significant_line):\n",
    "                        capturing = True\n",
    "                    \n",
    "                    # Stop capturing if 'Contents' is found\n",
    "                    if CONTENTS_PATTERN.match(first_significant_line):\n",
    "                        break\n",
    "                    \n",
    "                    # Append text if within summary section\n",
    "                    if capturing:\n",
    "                        summary_text += f\"[Page {page.page_number}]\\n\" + \"\\n\".join(text_without_header) + \"\\n\\n\"\n",
    "    \n",
    "    return summary_text\n",
    "\n",
    "pdf_text = extract_summary_section(f\"./reports_ie/{pdf_name}\", header_lines=1)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Chunk Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=2000, chunk_overlap=300):\n",
    "    \"\"\"\n",
    "    Splits text into smaller overlapping chunks using LangChain's text splitter.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Split the extracted text\n",
    "text_chunks = split_text_into_chunks(pdf_text)\n",
    "\n",
    "# Print the number of chunks and a sample chunk\n",
    "print(f\"Total chunks: {len(text_chunks)}\\nFirst chunk:\\n{text_chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant Chunk Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embeddings \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store text chunks into FAISS vector store\n",
    "vectorstore = FAISS.from_texts(text_chunks, embeddings)\n",
    "\n",
    "print(f\"Stored {len(text_chunks)} chunks in FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entities of interest that you'd like to extract chunks for from the vector store\n",
    "entities_of_interest = [\"accident type\", \"date\", \"time\", \"country\"]\n",
    "\n",
    "# Function for extracting most relevant chunks from vector store\n",
    "def find_most_relevant_chunks(entities, top_k):\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for each entity of interest\n",
    "    using FAISS similarity search and removes duplicates (if same chunk retrieved).\n",
    "    \n",
    "    Args:\n",
    "    - entities (list): List of entity names to query (e.g., [\"date\", \"location\"])\n",
    "    - top_k (int): Number of chunks to retrieve per entity\n",
    "    \n",
    "    Returns:\n",
    "    - unique_relevant_chunks (list): Deduplicated relevant chunks\n",
    "    \"\"\"\n",
    "    retrieved_chunks = set()  # Use a set to avoid duplicate chunks\n",
    "\n",
    "    for entity in entities:\n",
    "        print(f\"Searching for entity: {entity}\")\n",
    "        query = f\"Information about {entity}.\"\n",
    "        found_chunks = vectorstore.similarity_search(query, k=top_k)\n",
    "\n",
    "        for chunk in found_chunks:\n",
    "            retrieved_chunks.add(chunk.page_content)  # Add chunk if not already present\n",
    "\n",
    "    # Convert set back to a list and join into a single string\n",
    "    unique_relevant_chunks = list(retrieved_chunks)\n",
    "    combined_text = \"\\n\".join(unique_relevant_chunks)\n",
    "\n",
    "    print(f\"Found {len(unique_relevant_chunks)} unique relevant chunks.\")\n",
    "    return combined_text\n",
    "\n",
    "# Find & combine relevant chunks\n",
    "relevant_text = find_most_relevant_chunks(entities_of_interest, top_k=1)\n",
    "\n",
    "print(f\"\\nMost Relevant Chunks Combined:\\n{relevant_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key and model name\n",
    "MODEL=\"gpt-4o-mini\"\n",
    "\n",
    "# Load OpenAI API Key from requirements file\n",
    "with open(\"gpt-personal-key.txt\", \"r\") as file:\n",
    "    OPENAI_API_KEY = file.read().strip()\n",
    "\n",
    "# Instantiate OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating tokens\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in a given text for a specified OpenAI model.\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoder.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes for the entities extraction\n",
    "class Property(BaseModel):\n",
    "    \"\"\"A single property consisting of key and value.\"\"\"\n",
    "    key: str = Field(..., description=\"Property key\")\n",
    "    value: str = Field(..., description=\"Property value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    \"\"\"Represents an entity in the railway accident knowledge graph.\"\"\"\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    \"\"\"Represents a relationship between two entities in the graph.\"\"\"\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"A knowledge graph storing railway accident data.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(prompt, temperature=1):\n",
    "    \"\"\"\n",
    "    Calls the GPT model with the structured prompt and returns the raw response.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in analyzing railway accident reports. Return output in JSON format only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    response_text = completion.choices[0].message.content.strip()\n",
    "    response_text = re.sub(r'^```json\\n?|```$', '', response_text).strip()\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(text):\n",
    "    \"\"\"\n",
    "    Constructs a structured prompt to extract entities and relationships for railway accidents.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    Analyze the following railway accident report context and extract structured knowledge.\n",
    "\n",
    "    Return a JSON object with:\n",
    "    - `nodes`: A list of entities, specifically {entities_of_interest}.\n",
    "    - `rels`: A list of relationships linking entities.\n",
    "\n",
    "    Look at this example JSON response and follow the schema closely. Pay attention to date and type formats (e.g., EU date format, 24-hour time).\n",
    "    Ensure that the `source` and `target` nodes in `rels` are the same entities from the `nodes` list, and not different ones. \n",
    "    Think about the relationships between the entities, i.e., (node AccidentType occurred_at node Country, or node AccidentType has_date Date).\n",
    "    Make sure to map all nodes with other important entities, e.g., (node UniqueAccident has_date Date, node UniqueAccident occurred_at Country).\n",
    "    DO NOT map entities like (node Date is_date to node Time) or (node AccidentType is_type to node Country). This is incorrect.\n",
    "\n",
    "    {{\n",
    "        \"nodes\": [\n",
    "            {{\"id\": \"Dublin-Cork Accident\", \"type\": \"UniqueAccident\"}},\n",
    "            {{\"id\": \"Train Derailment\", \"type\": \"AccidentType\"}},\n",
    "            {{\"id\": \"23/12/2021\", \"type\": \"Date\"}}\n",
    "            {{\"id\": \"16:32\", \"type\": \"Time\"}},\n",
    "            {{\"id\": \"Ireland\", \"type\": \"Country\"}},\n",
    "            {{\"id\": \"European Rail Agency\", \"type\": \"RegulatoryBody\"}}\n",
    "            \n",
    "        ],\n",
    "        \"rels\": [\n",
    "            {{\"source\": \"Dublin-Cork Accident\", \"target\": \"Ireland\", \"type\": \"occurred_in\"}},\n",
    "            {{\"source\": \"Train Derailment\", \"target\": \"Dublin-Cork Accident\", \"type\": \"occurred_at\"}},\n",
    "            {{\"source\": \"Dublin-Cork Accident\", \"target\": \"European Rail Agency\", \"type\": \"investigated_by\"}},\n",
    "            {{\"source\": \"23/12/2021\", \"target\": \"Dublin-Cork Accident\", \"type\": \"has_date\"}},\n",
    "            {{\"source\": \"16:32\", \"target\": \"Dublin-Cork Accident\", \"type\": \"has_time\"}}\n",
    "\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Accident report context:\n",
    "    {text}\n",
    "\n",
    "    JSON:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_knowledge_graph(text):\n",
    "    \"\"\"\n",
    "    Extracts entities & relationships from a railway accident report using GPT.\n",
    "    - First, counts tokens and allows user decision.\n",
    "    - If within limit, runs GPT and handles errors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build prompt\n",
    "    prompt = build_prompt(text)\n",
    "\n",
    "    # Call GPT\n",
    "    response_text = call_gpt(prompt)\n",
    "\n",
    "    try:\n",
    "        extracted_graph = json.loads(response_text)  # Ensure valid JSON\n",
    "        return extracted_graph  # Successfully parsed knowledge graph\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON:\", str(e))\n",
    "        print(\"Storing raw response for review...\")\n",
    "\n",
    "        # Save the faulty response for debugging\n",
    "        with open(\"failed_graph_extractions.json\", \"a\") as file:\n",
    "            json.dump({\"input_text\": text[:1000], \"raw_output\": response_text}, file, indent=4)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "        return {}  # Return empty dictionary in case of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define token limit for function execution\n",
    "token_limit = 4096\n",
    "\n",
    "# Build the prompt and count tokens\n",
    "prompt = build_prompt(relevant_text)\n",
    "token_count = count_tokens(prompt)\n",
    "estimated_cost = token_count * 0.00000015  # Approximate OpenAI pricing\n",
    "\n",
    "# Check token limit\n",
    "if token_count > token_limit:\n",
    "    print(f\"Token count is too high: {token_count}\\nPlease reduce the chunk size or refine the prompt.\")\n",
    "else:\n",
    "    print(f\"Token count for prompt: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm Execution\n",
    "proceed = input(\"Do you want to proceed with knowledge graph extraction? (yes/no): \").strip().lower()\n",
    "if proceed != \"yes\":\n",
    "    print(\"Extraction aborted by user.\")\n",
    "else:\n",
    "    print(\"Sending request to GPT...\")\n",
    "    response_json = extract_knowledge_graph(pdf_text)\n",
    "\n",
    "response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to store DataFrame\n",
    "CSV_FILE = \"pdf_processing_results.csv\"\n",
    "\n",
    "def append_pdf_json_result(pdf_name, response_json):\n",
    "    \"\"\"\n",
    "    Appends the JSON output of response_json function to a DataFrame.\n",
    "    If the same PDF is processed again, it adds a new column (iteration).\n",
    "    If a new PDF is processed, it starts a new entry.\n",
    "    \"\"\"\n",
    "    # Load existing CSV if available, otherwise create a new DataFrame\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df = pd.read_csv(CSV_FILE)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"pdf_name\"])\n",
    "\n",
    "    # Check if PDF already exists in the DataFrame\n",
    "    existing_rows = df[df[\"pdf_name\"] == pdf_name]\n",
    "\n",
    "    # Convert JSON response to a string for storage\n",
    "    json_output = json.dumps(response_json, indent=2)\n",
    "\n",
    "    if not existing_rows.empty:\n",
    "        # Get all iteration columns for this PDF\n",
    "        iteration_columns = [col for col in df.columns if col.startswith(\"Iteration_\")]\n",
    "\n",
    "        # Ensure at least one iteration column exists before checking for duplicates\n",
    "        if iteration_columns:\n",
    "            # Check if this JSON already exists in any previous iterations\n",
    "            for iter_col in iteration_columns:\n",
    "                if iter_col in existing_rows.columns and not existing_rows[iter_col].isna().all():\n",
    "                    # Only compare if the column is not empty\n",
    "                    if existing_rows[iter_col].iloc[0] == json_output:\n",
    "                        print(f\"No changes in JSON across all iterations for {pdf_name}, skipping new entry.\")\n",
    "                        return df  # Exit without adding a duplicate entry\n",
    "\n",
    "        # Count how many previous iterations exist for this PDF\n",
    "        iteration_count = len(iteration_columns) + 1\n",
    "    else:\n",
    "        # New PDF file, start at iteration 1\n",
    "        iteration_count = 1\n",
    "\n",
    "    if not existing_rows.empty:\n",
    "        # Update existing row by adding a new column for this iteration\n",
    "        df.loc[df[\"pdf_name\"] == pdf_name, f\"Iteration_{iteration_count}\"] = json_output\n",
    "    else:\n",
    "        # Create a new row for the new PDF using pd.concat()\n",
    "        new_row = pd.DataFrame({\"pdf_name\": [pdf_name], f\"Iteration_{iteration_count}\": [json_output]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save DataFrame back to CSV\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "    print(f\"Successfully added {pdf_name} - Iteration {iteration_count} to results!\")\n",
    "    return df\n",
    "\n",
    "# Example execution\n",
    "results_df = append_pdf_json_result(pdf_name, response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the target PDF name and iteration number\n",
    "# pdf_query = \"IE-6262-200429 LC Collision XM240.pdf\"\n",
    "# iteration_number = 1 \n",
    "\n",
    "# # Construct the column name dynamically\n",
    "# iteration_column = f\"Iteration_{iteration_number}\"\n",
    "\n",
    "# # Extract the JSON string if the PDF exists and the iteration column is present\n",
    "# if pdf_query in results_df[\"pdf_name\"].values and iteration_column in results_df.columns:\n",
    "#     extracted_json = results_df.loc[results_df[\"pdf_name\"] == pdf_query, iteration_column].iloc[0]\n",
    "#     print(json.dumps(json.loads(extracted_json), indent=4))  # Print or return the JSON string\n",
    "# else:\n",
    "#     print(f\"No data found for {pdf_query} in {iteration_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Converts properties to a dictionary for graph storage.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "        return properties\n",
    "    for p in props:\n",
    "        properties[p[\"key\"]] = p[\"value\"]\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Maps extracted entities to graph nodes.\"\"\"\n",
    "    properties = {\"name\": node.id}\n",
    "    return BaseNode(\n",
    "        id=node.id,\n",
    "        type=node.type.capitalize(),\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Maps extracted relationships to graph edges.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j Connection Setup\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\"\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "try:\n",
    "    # Test the connection\n",
    "    with driver.session() as session:\n",
    "        session.run(\"RETURN 1\")\n",
    "    print(\"Connected to Neo4j successfully.\")\n",
    "except AuthError as e:\n",
    "    print(\"Authentication failed. Check your credentials:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear database\n",
    "def clear_neo4j_database():\n",
    "    \"\"\"Delete all nodes and relationships in the Neo4j database.\"\"\"\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    print(\"Neo4j database cleared successfully.\")\n",
    "\n",
    "# Run the function to clear the database\n",
    "clear_neo4j_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the JSON that you'd like to convert to a graph\n",
    "json_to_convert = json.loads(results_df[\"Iteration_1\"][5])\n",
    "print(f\"The JSON you chose:\\n{json_to_convert}\")\n",
    "\n",
    "# Index first row of a dataframe, first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_graph(json_to_convert, source_text):\n",
    "    \"\"\"\n",
    "    Converts extracted JSON into a graph-compatible format with correct entity types.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_node_type(json_data, node_id):\n",
    "        \"\"\"\n",
    "        Helper function to retrieve the correct node type from JSON.\n",
    "        \"\"\"\n",
    "        for node in json_data[\"nodes\"]:\n",
    "            if node[\"id\"] == node_id:\n",
    "                return node[\"type\"]\n",
    "        return \"Unknown\"  # Fallback if type is missing\n",
    "\n",
    "    if not json_to_convert:\n",
    "        print(\"No valid data to convert to a graph.\")\n",
    "        return None\n",
    "\n",
    "    # Convert Nodes\n",
    "    graph_nodes = [map_to_base_node(Node(id=node[\"id\"], type=node[\"type\"])) for node in json_to_convert[\"nodes\"]]\n",
    "\n",
    "    # Convert Relationships (Ensure correct types)\n",
    "    graph_rels = []\n",
    "    for rel in json_to_convert[\"rels\"]:\n",
    "        source_node = Node(id=rel[\"source\"], type=get_node_type(json_to_convert, rel[\"source\"]))\n",
    "        target_node = Node(id=rel[\"target\"], type=get_node_type(json_to_convert, rel[\"target\"]))\n",
    "        graph_rels.append(map_to_base_relationship(Relationship(source=source_node, target=target_node, type=rel[\"type\"])))\n",
    "\n",
    "    return GraphDocument(nodes=graph_nodes, relationships=graph_rels, source=Document(page_content=source_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_neo4j(graph_document):\n",
    "    \"\"\"\n",
    "    Stores extracted knowledge graph into Neo4j with dynamic labels.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Store nodes with dynamic labels\n",
    "        for node in graph_document.nodes:\n",
    "            session.run(f\"\"\"\n",
    "                MERGE (n:{node.type} {{id: $id}})\n",
    "                ON CREATE SET n.name = $name\n",
    "            \"\"\", id=node.id, name=node.id)\n",
    "\n",
    "        # Store relationships\n",
    "        for rel in graph_document.relationships:\n",
    "            session.run(\"\"\"\n",
    "                MATCH (s {id: $source})\n",
    "                MATCH (t {id: $target})\n",
    "                MERGE (s)-[:RELATIONSHIP {type: $type}]->(t)\n",
    "            \"\"\", source=rel.source.id, target=rel.target.id, type=rel.type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_railway_accident_report(json_to_convert):\n",
    "    \n",
    "    print(\"Converting JSON to graph format...\")\n",
    "    graph_document = convert_json_to_graph(json_to_convert, relevant_text)\n",
    "\n",
    "    if graph_document:\n",
    "        print(\"Graph structure created! Storing in Neo4j...\")\n",
    "        store_in_neo4j(graph_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store extracted entities into Neo4j\n",
    "try:\n",
    "    db_result = process_railway_accident_report(json_to_convert)\n",
    "    print(\"Data stored in Neo4j successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to store data in Neo4j:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Neo4j connection\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
