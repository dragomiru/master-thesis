{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\software\\miniconda\\envs\\master_thesis\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\software\\miniconda\\envs\\master_thesis\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\software\\miniconda\\envs\\master_thesis\\lib\\site-packages (from pandas) (2025.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\software\\miniconda\\envs\\master_thesis\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General \n",
    "import os\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# PDFs\n",
    "import pdfplumber\n",
    "import json\n",
    "import regex as re\n",
    "\n",
    "# LLMs\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import faiss\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    "    GraphDocument,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from pydantic import Field, BaseModel\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Neo4j\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j.exceptions import AuthError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file name\n",
    "pdf_name = \"IE-6262-200429 LC Collision XM240.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text_from_pdf(pdf_path, header_lines=1):\n",
    "#     \"\"\"\n",
    "#     Extracts text from a PDF, including removing headers from each page.\n",
    "#     \"\"\"\n",
    "#     text = []\n",
    "    \n",
    "#     with pdfplumber.open(pdf_path) as pdf:\n",
    "#         for i, page in enumerate(pdf.pages):\n",
    "#             page_text = page.extract_text()\n",
    "#             if page_text:\n",
    "#                 text_lines = page_text.split(\"\\n\")\n",
    "#                 text_without_header = \"\\n\".join(text_lines[header_lines:])\n",
    "#                 text.append(text_without_header)\n",
    "    \n",
    "#     return \"\\n\".join(text) # Returns a single string with all pages' text\n",
    "\n",
    "# # Apply extraction function\n",
    "# pdf_text = extract_text_from_pdf(f\"./reports_ie/{pdf_name}\", header_lines=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text_from_pdf(pdf_path, header_lines=1):\n",
    "#     \"\"\"\n",
    "#     Extracts text from a PDF while allowing for pre-processing,\n",
    "#     including removing headers from each page and skipping introduction pages.\n",
    "#     \"\"\"\n",
    "#     text = []\n",
    "\n",
    "#     skip_patterns = {\n",
    "#         \"roman_numerals\": r\"\\b[i|ii|iii|iv|v|vi|vii|viii|ix|x]+\\b\",  # Detect Roman numerals (intro pages)\n",
    "#         \"table_of_contents\": r\"\\b(contents)\\b\",  # Detects \"Table of Contents\" pages\n",
    "#     }\n",
    "\n",
    "#     with pdfplumber.open(pdf_path) as pdf:\n",
    "#         for page in pdf.pages:\n",
    "#             page_text = page.extract_text()\n",
    "#             if page_text:\n",
    "#                 text_lines = page_text.split(\"\\n\")\n",
    "#                 text_without_header = \"\\n\".join(text_lines[header_lines:])\n",
    "                \n",
    "#                 # Extract possible footer text (bottom 5 lines)\n",
    "#                 footer_text = \" \".join(text_lines[-5:]).strip().lower()\n",
    "                \n",
    "#                 # Check if the page contains any unwanted patterns\n",
    "#                 if any(re.search(pattern, footer_text) or re.search(pattern, page_text.lower()) \n",
    "#                        for pattern in skip_patterns.values()):\n",
    "#                     continue  # Skip unwanted pages\n",
    "                \n",
    "#                 # Append only valid report pages\n",
    "#                 text.append(f\"[Page {page.page_number}]\\n{text_without_header}\")\n",
    "    \n",
    "#     return \"\\n\".join(text)  # Returns a list where each item is a page's text\n",
    "\n",
    "# pdf_text = extract_text_from_pdf(f\"./reports_ie/{pdf_name}\", header_lines=2)\n",
    "# print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Page 4]\n",
      "Summary\n",
      "At approximately 13:40 hour (hrs) on the 29th April 2020, the 13:10 hrs passenger service from\n",
      "Westport to Dublin (Train A809) was approaching Kilnageer Level Crossing (LC) XM240,\n",
      "located approximately six kilometres (km) from Castlebar, County Mayo. At the same time a\n",
      "car approached LC XM240 with the gates open (left open by a previous user) and began\n",
      "travelling through LC XM240. When the driver of Train A809 (Driver A809) saw the car, he\n",
      "made a full-service brake application; however, the train could not stop in time and struck the\n",
      "car. Causal factors associated with this accident are:\n",
      "• The Car Driver failed to stop to look for trains on approach to LC XM240 as required by\n",
      "the Road Safety Authority’s (RSA) Rules of the Road, in part, as a result of the level\n",
      "crossing gates being open;\n",
      "• The sounding of the train horn was not effective at warning the Car Driver of the\n",
      "approaching train.\n",
      "A contributing factor to the accident was:\n",
      "• There is a high level of misuse and abuse at LC XM240, where the level crossing gates\n",
      "are continuously left open, despite laws being in place for them to be closed.\n",
      "The RAIU did not identify any systemic factors associated with this accident.\n",
      "The RAIU did not make any safety recommendations as a direct result of this accident.\n",
      "However, the RAIU noted that after the accident, a Decision Support System (DSS) was made\n",
      "operational at LC XM240, this impacts on stakeholders’ documentation. The RAIU also noted\n",
      "that further checks need to be conducted in relation to the sound pressure levels of the\n",
      "InterCity Railcar (ICR) train horns. As a result, the RAIU made four safety recommendations\n",
      "related to these additional observations:\n",
      "• 202101-01 – The RSA should update the “Rules of the Road” to include guidance on the\n",
      "DSS;\n",
      "• 202101-02 – Iarnród Éireann Infrastructure Manager (IÉ-IM) should update the ‘The SAFE\n",
      "use of Unattended Railway Level Crossings’ booklet to include guidance on the DSS;\n",
      "• 202101-03 – Iarnród Éireann Railway Undertaking (IÉ-RU) should put systems in place to\n",
      "ensure ICR train horns meet the current standards for sound pressure levels;\n",
      "• 202101-04 – The Commission for Railway Regulation (CRR) should review and update\n",
      "Section 5, Level Crossings, of their Guidelines for the Design of Railway Infrastructure and\n",
      "Rolling Stock, to ensure that guidance/reference on the DSS is included.\n",
      "Railway Accident Investigation Unit iii\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_summary_section(pdf_path, header_lines=1):\n",
    "    \"\"\"\n",
    "    Extracts the text from the pages between 'Summary' and 'Contents'.\n",
    "    \"\"\"\n",
    "    summary_text = \"\"\n",
    "    summary_pattern = r\"^summary\"  # Detects 'Summary' at the start\n",
    "    contents_pattern = r\"^contents\"  # Detects 'Contents' at the start\n",
    "    capturing = False  # Flag to start capturing text\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text_lines = page_text.split(\"\\n\")\n",
    "                text_without_header = text_lines[header_lines:]  # Remove header lines\n",
    "                \n",
    "                # Ensure there's enough content after the header\n",
    "                if text_without_header:\n",
    "                    first_significant_line = text_without_header[0].strip().lower()\n",
    "                    \n",
    "                    # Start capturing if 'Summary' is found\n",
    "                    if re.match(summary_pattern, first_significant_line):\n",
    "                        capturing = True\n",
    "                    \n",
    "                    # Stop capturing if 'Contents' is found\n",
    "                    if re.match(contents_pattern, first_significant_line):\n",
    "                        break\n",
    "                    \n",
    "                    # Append text if within summary section\n",
    "                    if capturing:\n",
    "                        summary_text += f\"[Page {page.page_number}]\\n\" + \"\\n\".join(text_without_header) + \"\\n\\n\"\n",
    "    \n",
    "    return summary_text\n",
    "\n",
    "pdf_text = extract_summary_section(f\"./reports_ie/{pdf_name}\", header_lines=1)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Chunk Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 2\n",
      "First chunk:\n",
      "[Page 4]\n",
      "Summary\n",
      "At approximately 13:40 hour (hrs) on the 29th April 2020, the 13:10 hrs passenger service from\n",
      "Westport to Dublin (Train A809) was approaching Kilnageer Level Crossing (LC) XM240,\n",
      "located approximately six kilometres (km) from Castlebar, County Mayo. At the same time a\n",
      "car approached LC XM240 with the gates open (left open by a previous user) and began\n",
      "travelling through LC XM240. When the driver of Train A809 (Driver A809) saw the car, he\n",
      "made a full-service brake application; however, the train could not stop in time and struck the\n",
      "car. Causal factors associated with this accident are:\n",
      "• The Car Driver failed to stop to look for trains on approach to LC XM240 as required by\n",
      "the Road Safety Authority’s (RSA) Rules of the Road, in part, as a result of the level\n",
      "crossing gates being open;\n",
      "• The sounding of the train horn was not effective at warning the Car Driver of the\n",
      "approaching train.\n",
      "A contributing factor to the accident was:\n",
      "• There is a high level of misuse and abuse at LC XM240, where the level crossing gates\n",
      "are continuously left open, despite laws being in place for them to be closed.\n",
      "The RAIU did not identify any systemic factors associated with this accident.\n",
      "The RAIU did not make any safety recommendations as a direct result of this accident.\n",
      "However, the RAIU noted that after the accident, a Decision Support System (DSS) was made\n",
      "operational at LC XM240, this impacts on stakeholders’ documentation. The RAIU also noted\n",
      "that further checks need to be conducted in relation to the sound pressure levels of the\n",
      "InterCity Railcar (ICR) train horns. As a result, the RAIU made four safety recommendations\n",
      "related to these additional observations:\n",
      "• 202101-01 – The RSA should update the “Rules of the Road” to include guidance on the\n",
      "DSS;\n",
      "• 202101-02 – Iarnród Éireann Infrastructure Manager (IÉ-IM) should update the ‘The SAFE\n",
      "use of Unattended Railway Level Crossings’ booklet to include guidance on the DSS;\n"
     ]
    }
   ],
   "source": [
    "def split_text_into_chunks(text, chunk_size=2000, chunk_overlap=300):\n",
    "    \"\"\"\n",
    "    Splits text into smaller overlapping chunks using LangChain's text splitter.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Split the extracted text\n",
    "text_chunks = split_text_into_chunks(pdf_text)\n",
    "\n",
    "# Print the number of chunks and a sample chunk\n",
    "print(f\"Total chunks: {len(text_chunks)}\\nFirst chunk:\\n{text_chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant Chunk Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embeddings \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2 chunks in FAISS.\n"
     ]
    }
   ],
   "source": [
    "# Store text chunks into FAISS vector store\n",
    "vectorstore = FAISS.from_texts(text_chunks, embeddings)\n",
    "\n",
    "print(f\"Stored {len(text_chunks)} chunks in FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for entity: accident type\n",
      "Searching for entity: date\n",
      "Searching for entity: time\n",
      "Searching for entity: country\n",
      "Found 2 unique relevant chunks.\n",
      "\n",
      "Most Relevant Chunks Combined:\n",
      "• 202101-01 – The RSA should update the “Rules of the Road” to include guidance on the\n",
      "DSS;\n",
      "• 202101-02 – Iarnród Éireann Infrastructure Manager (IÉ-IM) should update the ‘The SAFE\n",
      "use of Unattended Railway Level Crossings’ booklet to include guidance on the DSS;\n",
      "• 202101-03 – Iarnród Éireann Railway Undertaking (IÉ-RU) should put systems in place to\n",
      "ensure ICR train horns meet the current standards for sound pressure levels;\n",
      "• 202101-04 – The Commission for Railway Regulation (CRR) should review and update\n",
      "Section 5, Level Crossings, of their Guidelines for the Design of Railway Infrastructure and\n",
      "Rolling Stock, to ensure that guidance/reference on the DSS is included.\n",
      "Railway Accident Investigation Unit iii\n",
      "[Page 4]\n",
      "Summary\n",
      "At approximately 13:40 hour (hrs) on the 29th April 2020, the 13:10 hrs passenger service from\n",
      "Westport to Dublin (Train A809) was approaching Kilnageer Level Crossing (LC) XM240,\n",
      "located approximately six kilometres (km) from Castlebar, County Mayo. At the same time a\n",
      "car approached LC XM240 with the gates open (left open by a previous user) and began\n",
      "travelling through LC XM240. When the driver of Train A809 (Driver A809) saw the car, he\n",
      "made a full-service brake application; however, the train could not stop in time and struck the\n",
      "car. Causal factors associated with this accident are:\n",
      "• The Car Driver failed to stop to look for trains on approach to LC XM240 as required by\n",
      "the Road Safety Authority’s (RSA) Rules of the Road, in part, as a result of the level\n",
      "crossing gates being open;\n",
      "• The sounding of the train horn was not effective at warning the Car Driver of the\n",
      "approaching train.\n",
      "A contributing factor to the accident was:\n",
      "• There is a high level of misuse and abuse at LC XM240, where the level crossing gates\n",
      "are continuously left open, despite laws being in place for them to be closed.\n",
      "The RAIU did not identify any systemic factors associated with this accident.\n",
      "The RAIU did not make any safety recommendations as a direct result of this accident.\n",
      "However, the RAIU noted that after the accident, a Decision Support System (DSS) was made\n",
      "operational at LC XM240, this impacts on stakeholders’ documentation. The RAIU also noted\n",
      "that further checks need to be conducted in relation to the sound pressure levels of the\n",
      "InterCity Railcar (ICR) train horns. As a result, the RAIU made four safety recommendations\n",
      "related to these additional observations:\n",
      "• 202101-01 – The RSA should update the “Rules of the Road” to include guidance on the\n",
      "DSS;\n",
      "• 202101-02 – Iarnród Éireann Infrastructure Manager (IÉ-IM) should update the ‘The SAFE\n",
      "use of Unattended Railway Level Crossings’ booklet to include guidance on the DSS;\n"
     ]
    }
   ],
   "source": [
    "# Define entities of interest that you'd like to extract chunks for from the vector store\n",
    "entities_of_interest = [\"accident type\", \"date\", \"time\", \"country\"]\n",
    "\n",
    "# Function for extracting most relevant chunks from vector store\n",
    "def find_most_relevant_chunks(entities, top_k):\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for each entity of interest\n",
    "    using FAISS similarity search and removes duplicates (if same chunk retrieved).\n",
    "    \n",
    "    Args:\n",
    "    - entities (list): List of entity names to query (e.g., [\"date\", \"location\"])\n",
    "    - top_k (int): Number of chunks to retrieve per entity\n",
    "    \n",
    "    Returns:\n",
    "    - unique_relevant_chunks (list): Deduplicated relevant chunks\n",
    "    \"\"\"\n",
    "    retrieved_chunks = set()  # Use a set to avoid duplicate chunks\n",
    "\n",
    "    for entity in entities:\n",
    "        print(f\"Searching for entity: {entity}\")\n",
    "        query = f\"Information about {entity}.\"\n",
    "        found_chunks = vectorstore.similarity_search(query, k=top_k)\n",
    "\n",
    "        for chunk in found_chunks:\n",
    "            retrieved_chunks.add(chunk.page_content)  # Add chunk if not already present\n",
    "\n",
    "    # Convert set back to a list and join into a single string\n",
    "    unique_relevant_chunks = list(retrieved_chunks)\n",
    "    combined_text = \"\\n\".join(unique_relevant_chunks)\n",
    "\n",
    "    print(f\"Found {len(unique_relevant_chunks)} unique relevant chunks.\")\n",
    "    return combined_text\n",
    "\n",
    "# Find & combine relevant chunks\n",
    "relevant_text = find_most_relevant_chunks(entities_of_interest, top_k=3)\n",
    "\n",
    "print(f\"\\nMost Relevant Chunks Combined:\\n{relevant_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the API key and model name\n",
    "MODEL=\"gpt-4o-mini\"\n",
    "\n",
    "# Load OpenAI API Key from requirements file\n",
    "with open(\"gpt-personal-key.txt\", \"r\") as file:\n",
    "    OPENAI_API_KEY = file.read().strip()\n",
    "\n",
    "# Instantiate OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating tokens\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in a given text for a specified OpenAI model.\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoder.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Property(BaseModel):\n",
    "    \"\"\"A single property consisting of key and value.\"\"\"\n",
    "    key: str = Field(..., description=\"Property key\")\n",
    "    value: str = Field(..., description=\"Property value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    \"\"\"Represents an entity in the railway accident knowledge graph.\"\"\"\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    \"\"\"Represents a relationship between two entities in the graph.\"\"\"\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"A knowledge graph storing railway accident data.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(prompt, temperature=1):\n",
    "    \"\"\"\n",
    "    Calls the GPT model with the structured prompt and returns the raw response.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in analyzing railway accident reports. Return output in JSON format only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    response_text = completion.choices[0].message.content.strip()\n",
    "    response_text = re.sub(r'^```json\\n?|```$', '', response_text).strip()\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(text):\n",
    "    \"\"\"\n",
    "    Constructs a structured prompt to extract entities and relationships for railway accidents.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    Analyze the following railway accident report context and extract structured knowledge.\n",
    "\n",
    "    Return a JSON object with:\n",
    "    - `nodes`: A list of entities, specifically {entities_of_interest}.\n",
    "    - `rels`: A list of relationships linking entities.\n",
    "\n",
    "    Look at this example JSON response and follow the schema closely. Pay attention to date and type formats (e.g., EU date format, 24-hour time).\n",
    "    Ensure that the `source` and `target` nodes in `rels` are the same entities from the `nodes` list, and not different ones. And think about\n",
    "    the relationships between the entities, i.e., (node AccidentType occurred_at node Country, or node AccidentType has_date Date).\n",
    "    {{\n",
    "        \"nodes\": [\n",
    "            {{\"id\": \"Train Derailment\", \"type\": \"AccidentType\"}},\n",
    "            {{\"id\": \"23/12/2021\", \"type\": \"Date\"}}\n",
    "            {{\"id\": \"16:32\", \"type\": \"Time\"}},\n",
    "            {{\"id\": \"Ireland\", \"type\": \"Country\"}},\n",
    "            {{\"id\": \"European Rail Agency\", \"type\": \"RegulatoryBody\"}}\n",
    "            \n",
    "        ],\n",
    "        \"rels\": [\n",
    "            {{\"source\": \"Train Derailment\", \"target\": \"Ireland\", \"type\": \"occurred_at\"}},\n",
    "            {{\"source\": \"Train Derailment\", \"target\": \"European Rail Agency\", \"type\": \"investigated_by\"}}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Text:\n",
    "    {text}\n",
    "\n",
    "    JSON:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_knowledge_graph(text, token_limit=4096):\n",
    "    \"\"\"\n",
    "    Extracts entities & relationships from a railway accident report using GPT.\n",
    "    - First, counts tokens and allows user decision.\n",
    "    - If within limit, runs GPT and handles errors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Build Structured Graph Extraction Prompt\n",
    "    prompt = build_prompt(text)\n",
    "    \n",
    "    # Step 2: Count Tokens\n",
    "    token_count = count_tokens(prompt)\n",
    "    estimated_cost = token_count * 0.00000015  # Approximate OpenAI pricing\n",
    "\n",
    "    print(f\"Token Count for Prompt: {token_count} (Limit: {token_limit})\")\n",
    "    print(f\"Estimated Cost for {MODEL}: ${estimated_cost:.7f}\")\n",
    "\n",
    "    # Step 3: Check Token Limit\n",
    "    if token_count > token_limit:\n",
    "        print(\"Token count is too high! Please reduce the chunk size or refine the prompt.\")\n",
    "        return None  # Stop execution here\n",
    "\n",
    "    # Step 4: Confirm Execution\n",
    "    proceed = input(\"Do you want to proceed with knowledge graph extraction? (yes/no): \").strip().lower()\n",
    "    if proceed != \"yes\":\n",
    "        print(\"Extraction aborted by user.\")\n",
    "        return None  # Stop execution\n",
    "\n",
    "    print(\"Sending request to GPT...\")\n",
    "\n",
    "    # Step 5: Call GPT for Extraction\n",
    "    response_text = call_gpt(prompt)\n",
    "\n",
    "    # Step 6: Process Response & Handle JSON Errors\n",
    "    try:\n",
    "        extracted_graph = json.loads(response_text)  # Ensure valid JSON\n",
    "        return extracted_graph  # Successfully parsed knowledge graph\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON:\", str(e))\n",
    "        print(\"Storing raw response for review...\")\n",
    "\n",
    "        # Save the faulty response for debugging\n",
    "        with open(\"failed_graph_extractions.json\", \"a\") as file:\n",
    "            json.dump({\"input_text\": text[:1000], \"raw_output\": response_text}, file, indent=4)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "        return {}  # Return empty dictionary in case of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count for Prompt: 885 (Limit: 4096)\n",
      "Estimated Cost for gpt-4o-mini: $0.0001328\n",
      "Sending request to GPT...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'id': 'Train Collision', 'type': 'AccidentType'},\n",
       "  {'id': '29/04/2020', 'type': 'Date'},\n",
       "  {'id': '13:40', 'type': 'Time'},\n",
       "  {'id': 'Ireland', 'type': 'Country'}],\n",
       " 'rels': [{'source': 'Train Collision',\n",
       "   'target': 'Ireland',\n",
       "   'type': 'occurred_at'},\n",
       "  {'source': 'Train Collision', 'target': '29/04/2020', 'type': 'has_date'},\n",
       "  {'source': 'Train Collision', 'target': '13:40', 'type': 'has_time'}]}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json = extract_knowledge_graph(pdf_text)\n",
    "response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to store DataFrame\n",
    "CSV_FILE = \"pdf_processing_results.csv\"\n",
    "\n",
    "def append_pdf_json_result(pdf_name, response_json):\n",
    "    \"\"\"\n",
    "    Appends the JSON output of response_json function to a DataFrame.\n",
    "    If the same PDF is processed again, it adds a new column (iteration).\n",
    "    If a new PDF is processed, it starts a new entry.\n",
    "    \"\"\"\n",
    "    # Load existing CSV if available, otherwise create a new DataFrame\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df = pd.read_csv(CSV_FILE)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"pdf_name\"])\n",
    "\n",
    "    # Check if PDF already exists in the DataFrame\n",
    "    existing_rows = df[df[\"pdf_name\"] == pdf_name]\n",
    "    \n",
    "    if not existing_rows.empty:\n",
    "        # Count how many previous iterations exist for this PDF\n",
    "        iteration_count = sum(col.startswith(\"Iteration_\") for col in df.columns) + 1\n",
    "    else:\n",
    "        # New PDF file, start at iteration 1\n",
    "        iteration_count = 1\n",
    "\n",
    "    # Convert JSON response to a string for storage\n",
    "    json_output = json.dumps(response_json, indent=2)\n",
    "\n",
    "    # Get all iteration columns for this PDF\n",
    "    iteration_columns = [col for col in existing_rows.columns if col.startswith(\"Iteration_\")]\n",
    "\n",
    "    # Check if this JSON already exists in any previous iterations\n",
    "    if any(existing_rows[iter_col].iloc[0] == json_output for iter_col in iteration_columns):\n",
    "        print(\"No changes in JSON across all iterations, skipping new entry.\")\n",
    "        return df  # Exit without adding a duplicate entry\n",
    "\n",
    "    if not existing_rows.empty:\n",
    "        # Update existing row by adding a new column for this iteration\n",
    "        df.loc[df[\"pdf_name\"] == pdf_name, f\"Iteration_{iteration_count}\"] = json_output\n",
    "    else:\n",
    "        # Create a new row for the new PDF using pd.concat()\n",
    "        new_row = pd.DataFrame({\"pdf_name\": [pdf_name], f\"Iteration_{iteration_count}\": [json_output]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save DataFrame back to CSV\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes in JSON across all iterations, skipping new entry.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pdf_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Iteration_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Iteration_2",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "53c7c7ba-8a4c-46d3-81ac-ec877933d9e2",
       "rows": [
        [
         "0",
         "IE-6218-200111 Collision RRME Rosslare.pdf",
         "{\n  \"nodes\": [\n    {\n      \"id\": \"11/01/2020\",\n      \"type\": \"Date\"\n    },\n    {\n      \"id\": \"10:52\",\n      \"type\": \"Time\"\n    },\n    {\n      \"id\": \"Ireland\",\n      \"type\": \"Country\"\n    }\n  ],\n  \"rels\": [\n    {\n      \"source\": \"11/01/2020\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_in\"\n    },\n    {\n      \"source\": \"10:52\",\n      \"target\": \"Ireland\",\n      \"type\": \"reported_at\"\n    }\n  ]\n}",
         null
        ],
        [
         "1",
         "IE-6262-200429 LC Collision XM240.pdf",
         "{\n  \"nodes\": [\n    {\n      \"id\": \"29/04/2020\",\n      \"type\": \"Date\"\n    },\n    {\n      \"id\": \"13:40\",\n      \"type\": \"Time\"\n    },\n    {\n      \"id\": \"Ireland\",\n      \"type\": \"Country\"\n    }\n  ],\n  \"rels\": [\n    {\n      \"source\": \"29/04/2020\",\n      \"target\": \"13:40\",\n      \"type\": \"occurred_at\"\n    },\n    {\n      \"source\": \"29/04/2020\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_in\"\n    },\n    {\n      \"source\": \"13:40\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_in\"\n    }\n  ]\n}",
         "{\n  \"nodes\": [\n    {\n      \"id\": \"Train Collision\",\n      \"type\": \"AccidentType\"\n    },\n    {\n      \"id\": \"29/04/2020\",\n      \"type\": \"Date\"\n    },\n    {\n      \"id\": \"13:40\",\n      \"type\": \"Time\"\n    },\n    {\n      \"id\": \"Ireland\",\n      \"type\": \"Country\"\n    }\n  ],\n  \"rels\": [\n    {\n      \"source\": \"Train Collision\",\n      \"target\": \"Ireland\",\n      \"type\": \"occurred_at\"\n    },\n    {\n      \"source\": \"Train Collision\",\n      \"target\": \"29/04/2020\",\n      \"type\": \"has_date\"\n    },\n    {\n      \"source\": \"Train Collision\",\n      \"target\": \"13:40\",\n      \"type\": \"has_time\"\n    }\n  ]\n}"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>Iteration_1</th>\n",
       "      <th>Iteration_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IE-6218-200111 Collision RRME Rosslare.pdf</td>\n",
       "      <td>{\\n  \"nodes\": [\\n    {\\n      \"id\": \"11/01/202...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IE-6262-200429 LC Collision XM240.pdf</td>\n",
       "      <td>{\\n  \"nodes\": [\\n    {\\n      \"id\": \"29/04/202...</td>\n",
       "      <td>{\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train Col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     pdf_name  \\\n",
       "0  IE-6218-200111 Collision RRME Rosslare.pdf   \n",
       "1       IE-6262-200429 LC Collision XM240.pdf   \n",
       "\n",
       "                                         Iteration_1  \\\n",
       "0  {\\n  \"nodes\": [\\n    {\\n      \"id\": \"11/01/202...   \n",
       "1  {\\n  \"nodes\": [\\n    {\\n      \"id\": \"29/04/202...   \n",
       "\n",
       "                                         Iteration_2  \n",
       "0                                                NaN  \n",
       "1  {\\n  \"nodes\": [\\n    {\\n      \"id\": \"Train Col...  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = append_pdf_json_result(pdf_name, response_json)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"nodes\": [\n",
      "        {\n",
      "            \"id\": \"29/04/2020\",\n",
      "            \"type\": \"Date\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"13:40\",\n",
      "            \"type\": \"Time\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"Ireland\",\n",
      "            \"type\": \"Country\"\n",
      "        }\n",
      "    ],\n",
      "    \"rels\": [\n",
      "        {\n",
      "            \"source\": \"29/04/2020\",\n",
      "            \"target\": \"13:40\",\n",
      "            \"type\": \"occurred_at\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"29/04/2020\",\n",
      "            \"target\": \"Ireland\",\n",
      "            \"type\": \"occurred_in\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"13:40\",\n",
      "            \"target\": \"Ireland\",\n",
      "            \"type\": \"occurred_in\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Specify the target PDF name and iteration number\n",
    "pdf_query = \"IE-6262-200429 LC Collision XM240.pdf\"\n",
    "iteration_number = 1 \n",
    "\n",
    "# Construct the column name dynamically\n",
    "iteration_column = f\"Iteration_{iteration_number}\"\n",
    "\n",
    "# Extract the JSON string if the PDF exists and the iteration column is present\n",
    "if pdf_query in results_df[\"pdf_name\"].values and iteration_column in results_df.columns:\n",
    "    extracted_json = results_df.loc[results_df[\"pdf_name\"] == pdf_query, iteration_column].iloc[0]\n",
    "    print(json.dumps(json.loads(extracted_json), indent=4))  # Print or return the JSON string\n",
    "else:\n",
    "    print(f\"No data found for {pdf_query} in {iteration_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Converts properties to a dictionary for graph storage.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "        return properties\n",
    "    for p in props:\n",
    "        properties[p[\"key\"]] = p[\"value\"]\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Maps extracted entities to graph nodes.\"\"\"\n",
    "    properties = {\"name\": node.id}\n",
    "    return BaseNode(\n",
    "        id=node.id,\n",
    "        type=node.type.capitalize(),\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Maps extracted relationships to graph edges.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j Connection Setup\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\"\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "try:\n",
    "    # Test the connection\n",
    "    with driver.session() as session:\n",
    "        session.run(\"RETURN 1\")\n",
    "    print(\"Connected to Neo4j successfully.\")\n",
    "except AuthError as e:\n",
    "    print(\"Authentication failed. Check your credentials:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_neo4j_database():\n",
    "    \"\"\"Delete all nodes and relationships in the Neo4j database.\"\"\"\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    print(\"Neo4j database cleared successfully.\")\n",
    "\n",
    "# Run the function to clear the database\n",
    "clear_neo4j_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_graph(response_json, source_text):\n",
    "    \"\"\"\n",
    "    Converts extracted JSON into a graph-compatible format with nodes and relationships.\n",
    "    \"\"\"\n",
    "    if not response_json:\n",
    "        print(\"No valid data to convert to a graph.\")\n",
    "        return None\n",
    "\n",
    "    # Convert Nodes\n",
    "    graph_nodes = [map_to_base_node(Node(id=node[\"id\"], type=node[\"type\"])) for node in response_json[\"nodes\"]]\n",
    "\n",
    "    # Convert Relationships\n",
    "    graph_rels = []\n",
    "    for rel in response_json[\"rels\"]:\n",
    "        source_node = Node(id=rel[\"source\"], type=\"Unknown\")  # Temporary, type should be resolved\n",
    "        target_node = Node(id=rel[\"target\"], type=\"Unknown\")  # Temporary\n",
    "        graph_rels.append(map_to_base_relationship(Relationship(source=source_node, target=target_node, type=rel[\"type\"])))\n",
    "\n",
    "    # Create the structured GraphDocument with a source field\n",
    "    return GraphDocument(nodes=graph_nodes, relationships=graph_rels, source=Document(page_content=source_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_neo4j(graph_document):\n",
    "    \"\"\"\n",
    "    Stores extracted knowledge graph into Neo4j.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Store nodes\n",
    "        for node in graph_document.nodes:\n",
    "            session.run(\"\"\"\n",
    "                MERGE (n:Entity {id: $id, type: $type})\n",
    "                SET n.name = $name\n",
    "            \"\"\", id=node.id, type=node.type, name=node.id)\n",
    "\n",
    "        # Store relationships\n",
    "        for rel in graph_document.relationships:\n",
    "            session.run(\"\"\"\n",
    "                MATCH (s:Entity {id: $source})\n",
    "                MATCH (t:Entity {id: $target})\n",
    "                MERGE (s)-[:RELATIONSHIP {type: $type}]->(t)\n",
    "            \"\"\", source=rel.source.id, target=rel.target.id, type=rel.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_railway_accident_report(text):\n",
    "    \n",
    "    print(\"Converting JSON to graph format...\")\n",
    "    graph_document = convert_json_to_graph(response_json, relevant_text)\n",
    "\n",
    "    if graph_document:\n",
    "        print(\"Graph structure created! Storing in Neo4j...\")\n",
    "        store_in_neo4j(graph_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store extracted entities into Neo4j\n",
    "try:\n",
    "    db_result = process_railway_accident_report(response_json)\n",
    "    print(\"Data stored in Neo4j successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to store data in Neo4j:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Neo4j connection\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
