{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PDFs\n",
    "import pdfplumber\n",
    "\n",
    "# LLMs\n",
    "import textwrap\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.graphs.graph_document import (\n",
    "    GraphDocument,\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from openai import OpenAI\n",
    "import tiktoken \n",
    "\n",
    "# Neo4j\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j.exceptions import AuthError\n",
    "\n",
    "# Typing & Validation\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incident Report PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE-10375 - 210827 Collision with track equipment.pdf\n",
      "IE-10397 - 211207 Clontarf.pdf\n",
      "IE-10404 - 230222 Broken Rail Emly.pdf\n",
      "IE-200608 BnM Collision LC Offaly.pdf\n",
      "IE-6218-200111 Collision RRME Rosslare.pdf\n",
      "IE-6262-200429 LC Collision XM240.pdf\n",
      "IE-6291-200524 LC XA068 Ashfield.pdf\n",
      "IE-6305 - 200707_locomotive_224.pdf\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing PDFs\n",
    "pdf_directory = \"./reports_ie/\"\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [f for f in os.listdir(pdf_directory) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "for file in pdf_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file name that you want to process\n",
    "pdf_name = \"IE-6218-200111 Collision RRME Rosslare.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Page 6]\n",
      "Summary\n",
      "1 On Tuesday 21st February 2023 a T3 Possession was organised on the Up and Down\n",
      "lines of the Dublin to Cork mainline to allow for a track section, near Emly Level Crossing\n",
      "(LC), to undergo track maintenance.\n",
      "2 As part of the track maintenance, stressing and welding of the rails had to be undertaken\n",
      "in preparation for ballast cleaning.\n",
      "3 The stressing of the rails was being carried out on the Up line, which involved cutting both\n",
      "rails which was marked by a Iarnród Éireann Infras\n"
     ]
    }
   ],
   "source": [
    "# Define regex patterns to identify the start of the summary and contents sections\n",
    "SUMMARY_PATTERN = re.compile(r\"^summary\", re.IGNORECASE)\n",
    "CONTENTS_PATTERN = re.compile(r\"^contents\", re.IGNORECASE)\n",
    "\n",
    "# Define function to extract the summary section from Ireland reports\n",
    "def extract_summary_section(pdf_path: str, header_lines: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from 'Summary' to 'Contents' in an Irish rail report PDF.\n",
    "    If no summary section is found, returns the text from all pages.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"File not found: {pdf_path}\")\n",
    "\n",
    "    summary_text = \"\"\n",
    "    full_text = \"\"\n",
    "    capturing = False  \n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text_lines = page_text.split(\"\\n\")\n",
    "                text_without_header = text_lines[header_lines:]\n",
    "                page_content = f\"[Page {page.page_number}]\\n\" + \"\\n\".join(text_without_header) + \"\\n\\n\"\n",
    "\n",
    "                # Always append to full_text\n",
    "                full_text += page_content\n",
    "\n",
    "                if text_without_header:\n",
    "                    first_line = text_without_header[0].strip().lower()\n",
    "\n",
    "                    if SUMMARY_PATTERN.match(first_line):\n",
    "                        capturing = True\n",
    "                    \n",
    "                    if CONTENTS_PATTERN.match(first_line) and capturing:\n",
    "                        # Stop capturing when \"Contents\" is found after summary started.\n",
    "                        break\n",
    "                    \n",
    "                    if capturing:\n",
    "                        summary_text += page_content\n",
    "\n",
    "    if not summary_text:\n",
    "        print(f\"Warning: No summary section found in {pdf_path}. Returning full text.\")\n",
    "        return full_text\n",
    "\n",
    "    return summary_text\n",
    "\n",
    "# Example usage:\n",
    "pdf_text = extract_summary_section(f\"./reports_ie/{pdf_name}\", header_lines=1)\n",
    "print(pdf_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accident Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Broader",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1063d3fa-debc-4fd3-a47b-223c3601b6a1",
       "rows": [
        [
         "0",
         "A1",
         "A1",
         "Collisions",
         "A collision event falling within the A.1 sub-categories for which detailed information is not (yet) available.",
         "A"
        ],
        [
         "1",
         "A1-1 ",
         "A1.1 ",
         "Collision of train with a train/rail vehicle",
         "A front to front; front to end or a side collision between a part of a train and a part of another train or rail vehicle; or with shunting rolling stock.",
         "A1"
        ],
        [
         "2",
         "A1-2",
         "A1.2",
         "Collision of train with obstacle within the clearance gauge",
         "A collision between a part of a train and objects fixed or temporarily present on or near the track (except at level crossings if lost by a crossing vehicle or user); including collision with overhead contact lines.",
         "A1"
        ],
        [
         "3",
         "A1-3 ",
         "A1.3 ",
         "Collision of one or more rail vehicles with another rail vehicle",
         "Same as A1.1 but concerning more rail vehicles not forming a train.",
         "A1"
        ],
        [
         "4",
         "A1-4 ",
         "A1.4 ",
         "Collision of one or more rail vehicles with obstacle within the clearance gauge",
         "Same as A1.2 but concerning one or more rail vehicles not forming a train.",
         "A1"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Code</th>\n",
       "      <th>Name</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Broader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>A1</td>\n",
       "      <td>Collisions</td>\n",
       "      <td>A collision event falling within the A.1 sub-c...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1-1</td>\n",
       "      <td>A1.1</td>\n",
       "      <td>Collision of train with a train/rail vehicle</td>\n",
       "      <td>A front to front; front to end or a side colli...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1-2</td>\n",
       "      <td>A1.2</td>\n",
       "      <td>Collision of train with obstacle within the cl...</td>\n",
       "      <td>A collision between a part of a train and obje...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1-3</td>\n",
       "      <td>A1.3</td>\n",
       "      <td>Collision of one or more rail vehicles with an...</td>\n",
       "      <td>Same as A1.1 but concerning more rail vehicles...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1-4</td>\n",
       "      <td>A1.4</td>\n",
       "      <td>Collision of one or more rail vehicles with ob...</td>\n",
       "      <td>Same as A1.2 but concerning one or more rail v...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   Code                                               Name  \\\n",
       "0     A1     A1                                         Collisions   \n",
       "1  A1-1   A1.1        Collision of train with a train/rail vehicle   \n",
       "2   A1-2   A1.2  Collision of train with obstacle within the cl...   \n",
       "3  A1-3   A1.3   Collision of one or more rail vehicles with an...   \n",
       "4  A1-4   A1.4   Collision of one or more rail vehicles with ob...   \n",
       "\n",
       "                                          Definition Broader  \n",
       "0  A collision event falling within the A.1 sub-c...       A  \n",
       "1  A front to front; front to end or a side colli...      A1  \n",
       "2  A collision between a part of a train and obje...      A1  \n",
       "3  Same as A1.1 but concerning more rail vehicles...      A1  \n",
       "4  Same as A1.2 but concerning one or more rail v...      A1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load category A events\n",
    "cat_a_events = pd.read_csv(\"category-a-event-types-source.csv\", encoding='latin-1')\n",
    "cat_a_events.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Chunk Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incident Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 63\n",
      "First chunk:\n",
      "[Page 6]\n",
      "Summary\n",
      "1 On Tuesday 21st February 2023 a T3 Possession was organised on the Up and Down\n",
      "lines of the Dublin to Cork mainline to allow for a track section, near Emly Level Crossing\n",
      "(LC), to undergo track maintenance.\n",
      "2 As part of the track maintenance, stressing and welding of the rails had to be undertaken\n",
      "in preparation for ballast cleaning.\n",
      "3 The stressing of the rails was being carried out on the Up line, which involved cutting both\n",
      "rails which was marked by a Iarnród Éireann Infrastructure Manager (IÉ-IM) staff member,\n",
      "the Person in Charge of Stressing (this member of IÉ-IM staff was also the Track Delivery\n",
      "Unit Engineer (TDU Engineer) who supervised the works).\n",
      "4 A welding contractor was engaged to carry out the welding at the site location (110 miles\n",
      "355 yards), with a team comprising of a lead and second welder (the Welders) and a Weld\n",
      "Supervisor. Prior to the welding, the Welders placed clamps on either side of the first cut\n",
      "rail section and attached Rail Tensors to pull the rail ends together until the required\n",
      "welders gap was achieved. The rails were then anchored by the Chief Civil Engineer’s\n",
      "(CCE) Department staff to ensure no movement of the rails, and the rail ends were welded\n",
      "using the Thermit® SoW-5 welding process (to be referred to as Thermit SoW-5 for the\n",
      "remainder of the report); the first weld was “dropped” without issue.\n",
      "5 The Welders then placed the clamps on either side of the next cut location and attached\n",
      "the Rail Tensors in order to pull the rails ends together. On the first attempt when pressure\n",
      "was applied by the Rail Tensors the rails did not hold; on the second attempt, the Rail\n",
      "Tensors failed to pull the rail ends together; but, on the third attempt, the rail ends were\n",
      "pulled to the required welders gap. The Welders then waited to make sure the Rail Tensors\n",
      "held and once they were sure it was holding, the CCE staff fastened down the tension\n",
      "clamps to anchor the rails and the Welders dropped the weld without further issue.\n"
     ]
    }
   ],
   "source": [
    "def split_report_into_chunks(text: str, chunk_size: int = 2000, chunk_overlap: int = 300) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits text into smaller overlapping chunks using LangChain's text splitter.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        print(\"Warning: No text provided for splitting.\")\n",
    "        return []\n",
    "\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]  # If text is smaller than chunk size, return as single chunk\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Split the extracted text\n",
    "report_chunks = split_report_into_chunks(pdf_text)\n",
    "\n",
    "# Print the number of chunks and a sample chunk\n",
    "print(f\"Total chunks: {len(report_chunks)}\\nFirst chunk:\\n{report_chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Id': 'A1',\n",
       "  'Code': 'A1',\n",
       "  'Name': 'Collisions',\n",
       "  'Definition': 'A collision event falling within the A.1 sub-categories for which detailed information is not (yet) available.',\n",
       "  'Broader': 'A'},\n",
       " {'Id': 'A1-1 ',\n",
       "  'Code': 'A1.1 ',\n",
       "  'Name': 'Collision of train with a train/rail vehicle',\n",
       "  'Definition': 'A front to front; front to end or a side collision between a part of a train and a part of another train or rail vehicle; or with shunting rolling stock.',\n",
       "  'Broader': 'A1'},\n",
       " {'Id': 'A1-2',\n",
       "  'Code': 'A1.2',\n",
       "  'Name': 'Collision of train with obstacle within the clearance gauge',\n",
       "  'Definition': 'A collision between a part of a train and objects fixed or temporarily present on or near the track (except at level crossings if lost by a crossing vehicle or user); including collision with overhead contact lines.',\n",
       "  'Broader': 'A1'},\n",
       " {'Id': 'A1-3 ',\n",
       "  'Code': 'A1.3 ',\n",
       "  'Name': 'Collision of one or more rail vehicles with another rail vehicle',\n",
       "  'Definition': 'Same as A1.1 but concerning more rail vehicles not forming a train.',\n",
       "  'Broader': 'A1'},\n",
       " {'Id': 'A1-4 ',\n",
       "  'Code': 'A1.4 ',\n",
       "  'Name': 'Collision of one or more rail vehicles with obstacle within the clearance gauge',\n",
       "  'Definition': 'Same as A1.2 but concerning one or more rail vehicles not forming a train.',\n",
       "  'Broader': 'A1'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_a_events_dict = cat_a_events.to_dict(\"records\")\n",
    "cat_a_events_dict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 32\n",
      "First chunk:\n",
      "['Id: A1 Code: A1 Name: Collisions Definition: A collision event falling within the A.1 sub-categories for which detailed information is not (yet) available. Broader: A']\n"
     ]
    }
   ],
   "source": [
    "def split_events_into_chunks(data: list[dict], chunk_size: int = 2000, chunk_overlap: int = 300) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Converts each dictionary row into a string and splits each string into smaller overlapping chunks.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"Warning: No data provided for splitting.\")\n",
    "        return []\n",
    "\n",
    "    # Prepare the list of formatted strings (chunks) from the dictionary rows\n",
    "    chunk_list = [\n",
    "        f\"Id: {row['Id']} Code: {row['Code']} Name: {row['Name']} Definition: {row['Definition']} Broader: {row['Broader']}\"\n",
    "        for row in data\n",
    "    ]\n",
    "\n",
    "    # Function to split a single text into smaller chunks\n",
    "    def split_single_text(text: str) -> list[str]:\n",
    "        if not text:\n",
    "            print(\"Warning: No text provided for splitting.\")\n",
    "            return []\n",
    "\n",
    "        if len(text) <= chunk_size:\n",
    "            return [text]  # If text is smaller than chunk size, return as a single chunk\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        return text_splitter.split_text(text)\n",
    "\n",
    "    # Split each chunk string from the dictionary rows into smaller chunks\n",
    "    all_chunks = [split_single_text(chunk) for chunk in chunk_list]\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "event_chunks = split_events_into_chunks(cat_a_events_dict, chunk_size=2000, chunk_overlap=300)\n",
    "\n",
    "# Print the number of chunks and a sample chunk\n",
    "print(f\"Total chunks: {len(event_chunks)}\\nFirst chunk:\\n{event_chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant Chunk Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Miniconda\\envs\\master_thesis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Define embeddings \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incident Reports Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 63 chunks in FAISS.\n"
     ]
    }
   ],
   "source": [
    "# Store text chunks into FAISS vector store\n",
    "vectorstore = FAISS.from_texts(report_chunks, embeddings)\n",
    "\n",
    "print(f\"Stored {len(report_chunks)} chunks in FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for entity: unique accident\n",
      "Searching for entity: accident type\n",
      "Searching for entity: date\n",
      "Searching for entity: time\n",
      "Searching for entity: country\n",
      "Searching for entity: regulatory body\n",
      "\n",
      "Found 9 unique relevant chunks.\n",
      "\n",
      "Most Relevant Chunks Combined:\n",
      "[Page 33]\n",
      "113 At 07:56 hrs on Wednesday 23rd February 2023 the Signalman saw that the track\n",
      "circuit remained occupied after Train A205 passed through track circuit LJ789. The\n",
      "Signalman contacted Driver A205 to ensure the train had passed through the section\n",
      "safely.\n",
      "Events during the incident on 23rd February 2023\n",
      "114 At 07:57 hrs the Signalman contacted Mallow LCCC to see if there was a fault at Emly\n",
      "LC. Mallow LCCC confirmed there was no fault showing on their system and that Train\n",
      "A205 had gone through Emly LC.\n",
      "115 At 07:59 hrs the Signalman contacted the SET Department to report a track circuit fault\n",
      "at track circuit LJ789 on the Up Line.\n",
      "116 The SET staff members were deployed at approximately 08:10 hrs to the location of\n",
      "the track circuit fault.\n",
      "117 At 08:56 hrs the Signalman contacted Driver A303 (Train A303 Tralee to Dublin\n",
      "passenger service Up line) to explain that they would have to stop at Signal RC885\n",
      "because there was a track circuit fault on the Up Line. Driver A303 confirmed the\n",
      "instruction.\n",
      "118 At 09:03 hrs the Signalman contacted Driver A303 and asked them to examine the line\n",
      "from Signal LJ895 and the repeater Signal LJR353 as there was a track circuit fault. The\n",
      "Signalman explained that the Signal LJ895 may be showing danger (red aspect).\n",
      "119 At 09:15 hrs the SET staff members arrived at 110 MP checked the SET location case\n",
      "and found no faults, they left the location and travelled to the 109 MP.\n",
      "120 At 09:29 hrs, Driver A303 contacted the Signalman to say they were at Signal LJ895\n",
      "which was at danger. The Signalman authorised the movement to pass the signal at\n",
      "danger with further instructions to examine the line from Signal LJ895 to the repeater\n",
      "Signal LJR353 and to obey Signal LJR353. The Signalman told Driver A303 that they\n",
      "made contact with Mallow LCCC and that the barriers are down and safe for the passage\n",
      "of the Train A303 at Emly LC and that Driver A303 would have to contact the Mallow LCCC\n",
      "[Page 7]\n",
      "9 On Wednesday 22nd February 2023, at 07:56 hrs, the Signalman at Centralised Traffic\n",
      "Control (CTC) saw that a track circuit (LJ789 on the Up line, near Emly LC) remained\n",
      "occupied after the 07:00 hrs passenger service from Cork to Heuston (Train A205) passed\n",
      "through the location.\n",
      "10 The Signalman contacted the driver of Train A205 (Driver A205) to ensure the train had\n",
      "passed safely through the affected section of line.\n",
      "11 At 07:57 hrs the Signalman confirmed with Mallow Level Crossing Control Centre (LCCC)\n",
      "that there was no fault with the signalling equipment at Emly LC.\n",
      "12 At 07:59 hrs the Signalman contacted the Signal Electrical and Telecommunications (SET)\n",
      "Department to report the track circuit fault. SET staff members deployed to the location,\n",
      "and checked a number of SET location cases near the location of the fault; before walking\n",
      "the track and finding a broken rail, at the 110 miles 355 yards (the location of the welded\n",
      "rail), at 10:35 hrs.\n",
      "13 Just as the SET staff member had discovered the broken rail a train was approaching on\n",
      "the Up line. It was the 09:25 hrs passenger service from Cork to Heuston (Train A209)\n",
      "travelling under caution. The SET staff member stepped out to a position of safety and\n",
      "signalled the train to stop. Train A209 travelled over the broken rail before coming to a\n",
      "stop.\n",
      "14 The SET member spoke to the driver (Driver A209) and told them the fault was a broken\n",
      "rail and the train had travelled over it. Driver A209 notified the Signalman of the broken rail\n",
      "and resumed their journey.\n",
      "15 The Signalman contacted the SET staff member who confirmed that the track circuit fault\n",
      "was as a result of a broken rail.\n",
      "16 The Signalman took the appropriate actions and signal protection was put in place on the\n",
      "Up line.\n",
      "17 The Signalman contacted the Permanent Way Inspector (PWI) who confirmed that they\n",
      "had been notified of the broken rail. The PWI attended the site of the broken rail, and the\n",
      "rail was clamped and plated.\n",
      "[Page 41]\n",
      "Measures taken by the Welding Contractor since the incident\n",
      "152 A new standard of maintenance had been adapted since the 1st March 2023 with\n",
      "servicing of the Rail Tensors, and associated equipment, every three months. Records of\n",
      "the servicing schedule are up to date with each Rail Tensors having a serving label\n",
      "attached with the date it was last serviced and the next date it is due for service.\n",
      "153 At the time of the publication of this report all six Rail Tensors, owned by the Welding\n",
      "Contractor had been recalibrated, by an external firm specialising in hydraulic systems.\n",
      "154 The Welding Contractor has ordered metal covers, designed to fit over the top of the\n",
      "mould, to prevent luting sand entering the mould while the welders are preparing for\n",
      "Thermit SoW-5.\n",
      "Measures taken by the CRR since the incident\n",
      "155 The CRR undertake a variety of supervision activities on railway organisations which\n",
      "comprise of audits, inspections (both planned and following occurrences) and meetings\n",
      "with railway organisation personnel.\n",
      "156 Specific scheduled meetings include periodic safety performance review meetings,\n",
      "CRR outcome review meetings and RAIU Safety recommendations review meetings, etc.\n",
      "CRR Supervision activities are both proactive (planned) and reactive (in response to\n",
      "something such as an accident/ incident) as per the activity types outlined above and\n",
      "commensurate with the levels of risk involved.\n",
      "157 As part of the CRR's Supervision Plan for 2024 on IÉ-IM an inspection is planned which\n",
      "will encompass sampling on-site welding at multiple sites in order to check site welding\n",
      "processes and implemented risk mitigations across these sites. It should be noted that this\n",
      "activity while envisaged to be undertaken in 2024 will depend on reactive supervision\n",
      "activities which receive prioritisation with respect to resource allocation.\n",
      "Railway Accident Investigation Unit 36\n",
      "[Page 13]\n",
      "Communications & evidence collection\n",
      "33 Communications were conducted through established processes (such as RFIs).\n",
      "34 Relevant stakeholders were issued the draft investigation report for comment, comments\n",
      "were reviewed and responses on their comments returned. In this instance the\n",
      "stakeholders were: IÉ-IM (parties and roles), the Welding Contractor, the Third Party\n",
      "Contractor and the Commission for Railway Regulation (CRR)2.\n",
      "35 All relevant parties co-operated fully with the RAIU investigation; with no difficulties arising.\n",
      "Other stakeholder inputs\n",
      "36 No other parties, such as the emergency services, were required for this incident.\n",
      "2 The CRR is the National Safety Authority (NSA) for the Republic of Ireland and is responsible\n",
      "for the regulatory oversight of the Safety Management System (SMS) and enforcement of\n",
      "railway safety in the Republic of Ireland in accordance with the Railway Safety Act 2005 and\n",
      "the European Railway Safety Directive.\n",
      "Railway Accident Investigation Unit 8\n",
      "[Page 16]\n",
      "45 The IÉ-IM CCE roles involved, directly and indirectly, and respective experiences at the\n",
      "incident, are as follows:\n",
      "• SET staff members – Attended the site due to the reported fault, found the broken rail\n",
      "and contacted the Signalman; They have their own department;\n",
      "• PWI – Responded to the call attended the site to examine the broken rail and arranged\n",
      "the subsequent response to the broken rail;\n",
      "• Person in Charge of Stressing – A member of the CCE Department (TDU Engineer)\n",
      "who was on site on the night the rails were cut, stressed, welded, and tested. The staff\n",
      "member was also the nominated Person in Charge of Stressing and was competent\n",
      "for this role, as set out in CCE-TMS-422;\n",
      "• PICOP – They handed back the T3 Possession to the Signalman when all works had\n",
      "been completed on the night.\n",
      "46 The IÉ-IM Operations roles involved, directly and indirectly, and respective experiences at\n",
      "the incident, are as follows:\n",
      "• The Signalman – The Main Line Signalman at CTC who responded to the track circuit\n",
      "fault confirmed with Mallow LCCC the line was clear and notified SET.\n",
      "IÉ-RU\n",
      "47 IÉ-RU is the railway undertaking who owns and operates mainline and suburban railway\n",
      "services in Ireland and operates under a safety certificate issued by the CRR. The RU\n",
      "Safety Certificate is issued in conformity with European Directive 2004/49/EC and S.I. 249\n",
      "of 2015; the Safety Certificate was renewed on 23rd March 2018 for a period of five years\n",
      "(valid at the time of incident). The IÉ-RU roles involved in this incident and relevant to this\n",
      "investigation are:\n",
      "• Driver A303 – Driver of Train A303 and was competent to drive the train at the time of\n",
      "the incident;\n",
      "• Driver A209 – Driver of Train A209 and was competent to drive the train at the time of\n",
      "the incident.\n",
      "Railway Accident Investigation Unit 11\n",
      "[Page 31]\n",
      "Events before, during and after the incident\n",
      "Events before the incident\n",
      "97 Engineering works (with six separate worksite) were scheduled from Monday 20th to\n",
      "Sunday the 26th February 2023 on the Up and Down Line from Dublin to Cork.\n",
      "98 One of the T3 Possessions was the section of track from Thurles to Charleville to facilitate\n",
      "track maintenance on the Up and Down lines of the Dublin to Cork mainline as part of a\n",
      "pre-planned programme of engineering works.\n",
      "99 On Tuesday 21st February 2023, the track section near Emly LC was being prepared for\n",
      "ballast cleaning. The stressing of the rails was undertaken at this stage to eliminate the\n",
      "need for a speed restriction after the ballast cleaning had been carried out.\n",
      "100 The stressing was being carried out on a reverse curve on the Up Line; this meant that\n",
      "both rails (the rail nearest the six foot and the rail nearest the cess) were cut at pulling\n",
      "point locations marked by the Person in Charge of Stressing who also supervised the\n",
      "works. The rail temperatures of 8˚C was recorded for both weld locations.\n",
      "101 The Welders placed clamps on either side of the cut rail section (on the rail nearest\n",
      "the six foot) and attached a Rail Tensors. The rail ends were pulled together until the\n",
      "required welders gap was achieved. The pressure reading on the Rail Tensors was 5000\n",
      "psi this was the pressure required to pull the rails to the required welders gap. The valve\n",
      "was closed off. The rails were refastened to the sleepers and prepared for Thermit SoW-\n",
      "5. The weld was dropped at 03:24 hrs.\n",
      "102 The Welders placed clamps on either side of the cut rail (the rail nearest the cess).\n",
      "103 On the first attempt when the rails were pulled and the valve was closed off on the Rail\n",
      "Tensors to maintain pressure, the Person in Charge of Stressing noticed a slight\n",
      "movement in the rail. The valve was opened, and the pressure released. The rail was\n",
      "unfastened from the sleepers.\n",
      "[Page 45]\n",
      "Additional Information\n",
      "List of abbreviations\n",
      "AO Additional Observations\n",
      "CaF Causal Factor\n",
      "CAWS Continuous Automatic Warning System\n",
      "CCE Chief Civil Engineer\n",
      "CME Chief Mechanical Engineer\n",
      "CoF Contributory Factor\n",
      "CRR Commission for Railway Regulation\n",
      "CTC Centralised Traffic Control\n",
      "CWR Continuous Welded Rail\n",
      "DART Dublin Area Rapid Transport\n",
      "DMU Diesel Multiple Units\n",
      "EU European Union\n",
      "ESR Emergency Speed Restriction\n",
      "hr hour\n",
      "IAMS Infrastructure Asset Management System\n",
      "ICR InterCity Railcar\n",
      "IÉ-IM Iarnród Éireann Infrastructure Manager\n",
      "IÉ-RU Iarnród Éireann Railway Undertaking\n",
      "km kilometre\n",
      "km/h kilometres per hour\n",
      "LC Level Crossing\n",
      "LCCC Level Crossing Control Centre\n",
      "m metre\n",
      "mm millimetres\n",
      "MP Milepost\n",
      "mph miles per hour\n",
      "NSA National Safety Authority\n",
      "PICOP Person In Charge Of the Possession\n",
      "psi pound-force per square inch\n",
      "PWI Permanent Way Inspector\n",
      "RAIU Railway Accident Investigation Unit\n",
      "RFI Request For Information\n",
      "SET Signal Electrical and Telecommunications\n",
      "SF Systemic Factor\n",
      "Railway Accident Investigation Unit 40\n",
      "\n",
      "[Page 46]\n",
      "SFT Stress Free Temperature\n",
      "SMS Safety Management System\n",
      "TCB Track Circuit Block\n",
      "TDU Track Delivery Unit\n",
      "TSR Temporary Speed Restriction\n",
      "Railway Accident Investigation Unit 41\n",
      "[Page 49]\n",
      "contractions, inadequate welding temperature and improper\n",
      "solidification.\n",
      "Incident Any occurrence, other than an accident or serious accident, associated\n",
      "with the operation of trains and affecting the safety of operation. For\n",
      "heavy rail, the EU Agency for Railways divides incidents into the\n",
      "following categories: infrastructure; energy; control-command &\n",
      "signalling; rolling stock; traffic operations & management and others.\n",
      "Infrastructure Is the internal Geographic Information System used by Iarnród\n",
      "Asset Éireann in the mapping and management of infrastructure assets.\n",
      "Management\n",
      "System\n",
      "Investigation A process conducted for the purpose of accident and incident\n",
      "prevention which includes the gathering and analysis of information,\n",
      "the drawing of conclusions, including the determination of causes and,\n",
      "when appropriate, the making of safety recommendations\n",
      "Location cases Accommodate railway signalling equipment to detect the location of\n",
      "trains, control the trackside signals and switch the points, etc.\n",
      "Luting Sand Sand used to seal gaps around moulds.\n",
      "Mile Post Marks distances.\n",
      "Person in charge They are the competent person nominated to manage the possession\n",
      "of the possession to ensure the safe and correct establishment of the protection of the\n",
      "possession. Managing access to the Possession area by Engineering\n",
      "Supervisors. Managing the establishment of Engineering Work Sites\n",
      "within the possession. Liaising with the Signalman regarding the\n",
      "passage of the Train in and out of Possession. Controlling the\n",
      "movement of the Train between the Protection and Work Sites.\n",
      "Ensuring that all the forgoing is correctly removed in reverse sequence,\n",
      "the Possession is relinquished, and the Line handed back to the\n",
      "Signalman at the due time.\n",
      "Permanent Way Responsible for overseeing and guiding workplace activities in his CCE\n",
      "Inspector location. PWIs shall be familiar with the content of this technical\n",
      "standard and the requirements for the stressing of CWR.\n",
      "for Information (RFIs) to the IÉ-IM Safety Department and formal interviewing of relevant\n",
      "staff. Related to this investigation, the RAIU collated and logged the following evidence:\n",
      "• Photographs taken on the day from the site;\n",
      "• Witness statements and interview notes from parties involved in the incident;\n",
      "• The planning and execution of the work being carried out;\n",
      "Railway Accident Investigation Unit 6\n"
     ]
    }
   ],
   "source": [
    "# Define entities of interest that you'd like to extract chunks for from the vector store\n",
    "entities_of_interest = [\"unique accident\", \"accident type\", \"date\", \"time\", \"country\", \"regulatory body\"]\n",
    "\n",
    "# Function for extracting most relevant chunks from vector store\n",
    "def find_most_relevant_chunks(entities: list[str], top_k: int) -> str:\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for each entity of interest\n",
    "    using FAISS similarity search and removes duplicates (if same chunk retrieved).\n",
    "    \n",
    "    Args:\n",
    "    - entities (list): List of entity names to query (e.g., [\"date\", \"location\"])\n",
    "    - top_k (int): Number of chunks to retrieve per entity\n",
    "    \n",
    "    Returns:\n",
    "    - unique_relevant_chunks (list): Deduplicated relevant chunks\n",
    "    \"\"\"\n",
    "    retrieved_chunks = set()  # Use a set to avoid duplicate chunks\n",
    "\n",
    "    for entity in entities:\n",
    "        print(f\"Searching for entity: {entity}\")\n",
    "        query = f\"Report details about {entity}.\"\n",
    "        found_chunks = vectorstore.similarity_search(query, k=top_k)\n",
    "\n",
    "        for chunk in found_chunks:\n",
    "            retrieved_chunks.add(chunk.page_content)  # Add chunk if not already present\n",
    "\n",
    "    # Convert set back to a list and join into a single string\n",
    "    unique_relevant_chunks = list(retrieved_chunks)\n",
    "    combined_text = \"\\n\".join(unique_relevant_chunks)\n",
    "\n",
    "    print(f\"\\nFound {len(unique_relevant_chunks)} unique relevant chunks.\")\n",
    "    return combined_text\n",
    "\n",
    "# Find & combine relevant chunks\n",
    "relevant_report_text = find_most_relevant_chunks(entities_of_interest, top_k=3)\n",
    "\n",
    "print(f\"\\nMost Relevant Chunks Combined:\\n{relevant_report_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Events Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 32 chunks in FAISS.\n"
     ]
    }
   ],
   "source": [
    "flat_event_chunks = [chunk for sublist in event_chunks for chunk in sublist]\n",
    "\n",
    "# Store text chunks into FAISS vector store\n",
    "vectorstore = FAISS.from_texts(flat_event_chunks, embeddings)\n",
    "\n",
    "print(f\"Stored {len(flat_event_chunks)} chunks in FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: A1-1  Code: A1.1  Name: Collision of train with a train/rail vehicle Definition: A front to front; front to end or a side collision between a part of a train and a part of another train or rail vehicle; or with shunting rolling stock. Broader: A1\n",
      "Id: A1 Code: A1 Name: Collisions Definition: A collision event falling within the A.1 sub-categories for which detailed information is not (yet) available. Broader: A\n",
      "Id: A1-3  Code: A1.3  Name: Collision of one or more rail vehicles with another rail vehicle Definition: Same as A1.1 but concerning more rail vehicles not forming a train. Broader: A1\n",
      "Id: A1-4  Code: A1.4  Name: Collision of one or more rail vehicles with obstacle within the clearance gauge Definition: Same as A1.2 but concerning one or more rail vehicles not forming a train. Broader: A1\n",
      "Id: A1-2 Code: A1.2 Name: Collision of train with obstacle within the clearance gauge Definition: A collision between a part of a train and objects fixed or temporarily present on or near the track (except at level crossings if lost by a crossing vehicle or user); including collision with overhead contact lines. Broader: A1\n"
     ]
    }
   ],
   "source": [
    "accident_type = \"Collision\"\n",
    "\n",
    "def find_most_relevant_chunks(top_k: int) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant chunks from the vector store based on the query.\n",
    "    \"\"\"\n",
    "    query = f\"{accident_type}\"\n",
    "    \n",
    "    # Perform the similarity search\n",
    "    found_chunks = vectorstore.similarity_search(query, k=top_k)\n",
    "    \n",
    "    # Extract the text from each document in the list\n",
    "    found_chunks = [doc.page_content for doc in found_chunks]\n",
    "    \n",
    "    # Join the list of texts into a single string\n",
    "    found_chunks = \"\\n\".join(found_chunks)\n",
    "    \n",
    "    return found_chunks\n",
    "\n",
    "# Find & combine relevant chunks\n",
    "relevant_events_text = find_most_relevant_chunks(top_k=5)\n",
    "\n",
    "print(f\"{relevant_events_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key and model name\n",
    "MODEL_GPT = \"gpt-4o-mini\"\n",
    "\n",
    "# Load OpenAI API Key from requirements file\n",
    "with open(\"gpt-personal-key.txt\", \"r\") as file:\n",
    "    OPENAI_API_KEY = file.read().strip()\n",
    "\n",
    "# Instantiate OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating tokens\n",
    "def count_tokens(text: str, model: str = \"gpt-4o\") -> int:\n",
    "    \"\"\"Efficiently counts tokens in a text for a given OpenAI model.\"\"\"\n",
    "    if model not in count_tokens.encoders:\n",
    "        count_tokens.encoders[model] = tiktoken.encoding_for_model(model)\n",
    "    return len(count_tokens.encoders[model].encode(text))\n",
    "\n",
    "count_tokens.encoders = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes for the entities extraction\n",
    "class Property(BaseModel):\n",
    "    \"\"\"A single property consisting of key and value.\"\"\"\n",
    "    key: str = Field(..., description=\"Property key\")\n",
    "    value: str = Field(..., description=\"Property value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    \"\"\"Represents an entity in the railway accident knowledge graph.\"\"\"\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    \"\"\"Represents a relationship between two entities in the graph.\"\"\"\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"A knowledge graph storing railway accident data.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to call GPT model with respective prompt\n",
    "def call_gpt(prompt, temperature=1):\n",
    "    \"\"\"\n",
    "    Calls the GPT model with the structured prompt and returns the raw response.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in analyzing railway accident reports. Return output in JSON format only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    response_text = completion.choices[0].message.content.strip()\n",
    "    response_text = re.sub(r'^```json\\n?|```$', '', response_text).strip()\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gpt_prompt(text):\n",
    "    \"\"\"\n",
    "    Constructs a structured prompt to extract entities and relationships for railway accidents.\n",
    "    \"\"\"\n",
    "    \n",
    "    schema_example = \"\"\"\n",
    "    {\n",
    "        \"nodes\": [\n",
    "            {\"id\": \"Dublin-Cork Accident\", \"type\": \"UniqueAccident\"},\n",
    "            {\"id\": \"Train Derailment\", \"type\": \"AccidentType\"},\n",
    "            {\"id\": \"23/12/2021\", \"type\": \"Date\"},\n",
    "            {\"id\": \"16:32\", \"type\": \"Time\"},\n",
    "            {\"id\": \"Ireland\", \"type\": \"Country\"},\n",
    "            {\"id\": \"European Rail Agency\", \"type\": \"RegulatoryBody\"}\n",
    "        ],\n",
    "        \"rels\": [\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"Ireland\", \"type\": \"occurred_in\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"Collision\", \"type\": \"is_type\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"European Rail Agency\", \"type\": \"investigated_by\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"23/12/2021\", \"type\": \"has_date\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"16:32\", \"type\": \"has_time\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "    Analyze the following railway accident report context and extract structured knowledge.\n",
    "\n",
    "    Return a JSON object with:\n",
    "    - `nodes`: A list of entities, specifically {entities_of_interest}.\n",
    "    - `rels`: A list of relationships linking entities.\n",
    "\n",
    "    Look at the JSON schema example response and follow it closely. Pay attention to date and type formats (e.g., EU date format, 24-hour time).\n",
    "    Ensure that the `source` and `target` nodes in `rels` are the same entities from the `nodes` list, and not different ones. \n",
    "    Make sure to map all nodes with other important entities, e.g., (node UniqueAccident has_date node Date, node UniqueAccident occurred_at node Country).\n",
    "    DO NOT map entities like (node Date is_date to node Time) or (node AccidentType is_type to node Country). This is incorrect.\n",
    "    The `type` field in `rels` should be a verb phrase (e.g., \"occurred_in\", \"investigated_by\").\n",
    "    And the `id` field in `nodes` should be the exact text of the entity, not a description or a summary.\n",
    "\n",
    "    Schema example:\n",
    "    {schema_example}\n",
    "\n",
    "    Accident report context:\n",
    "    {text}\n",
    "\n",
    "    JSON:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_knowledge_graph(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts entities & relationships from a railway accident report using GPT.\n",
    "    - First, counts tokens and allows user decision.\n",
    "    - If within limit, runs GPT and handles errors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build prompt\n",
    "    prompt = build_gpt_prompt(text)\n",
    "\n",
    "    # Call GPT\n",
    "    response_text = call_gpt(prompt)\n",
    "\n",
    "    try:\n",
    "        extracted_graph = json.loads(response_text)  # Ensure valid JSON\n",
    "        return extracted_graph  # Successfully parsed knowledge graph\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON:\", str(e))\n",
    "        print(\"Storing raw response for review...\")\n",
    "\n",
    "        # Save the faulty response for debugging\n",
    "        with open(\"failed_graph_extractions.json\", \"a\") as file:\n",
    "            json.dump({\"input_text\": text[:1000], \"raw_output\": response_text}, file, indent=4)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "        return {}  # Return empty dictionary in case of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define token limit for function execution\n",
    "token_limit = 4096\n",
    "\n",
    "# Build the prompt and count tokens\n",
    "prompt = build_gpt_prompt(relevant_report_text)\n",
    "token_count = count_tokens(prompt)\n",
    "estimated_cost = token_count * 0.00000015  # Approximate OpenAI pricing\n",
    "\n",
    "# Check token limit\n",
    "if token_count > token_limit:\n",
    "    print(f\"Token count is too high: {token_count}\\nPlease reduce the chunk size or refine the prompt.\")\n",
    "    print(f\"Estimated cost: ${estimated_cost:.5f}\")\n",
    "else:\n",
    "    print(f\"Token count for prompt: {token_count}\")\n",
    "    print(f\"Estimated cost: ${estimated_cost:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm Execution\n",
    "proceed = input(\"Do you want to proceed with information extraction? (yes/no): \").strip().lower()\n",
    "if proceed != \"yes\":\n",
    "    print(\"Extraction aborted by user.\")\n",
    "else:\n",
    "    print(\"Sending request to GPT...\")\n",
    "    gpt_response = extract_knowledge_graph(relevant_report_text)\n",
    "    response_dict = {\"model\": MODEL_GPT, \"response\": gpt_response}\n",
    "    \n",
    "gpt_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_local_prompt(text):\n",
    "    \"\"\"\n",
    "    Constructs a structured prompt to extract entities and relationships for railway accidents.\n",
    "    \"\"\"\n",
    "    \n",
    "    schema_example = \"\"\"\n",
    "    {\n",
    "        \"nodes\": [\n",
    "            {\"id\": \"Dublin-Cork Accident\", \"type\": \"UniqueAccident\"},\n",
    "            {\"id\": \"Train Derailment\", \"type\": \"AccidentType\"},\n",
    "            {\"id\": \"23/12/2021\", \"type\": \"Date\"},\n",
    "            {\"id\": \"16:32\", \"type\": \"Time\"},\n",
    "            {\"id\": \"Ireland\", \"type\": \"Country\"},\n",
    "            {\"id\": \"European Rail Agency\", \"type\": \"RegulatoryBody\"}\n",
    "        ],\n",
    "        \"rels\": [\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"Ireland\", \"type\": \"occurred_in\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"Collision\", \"type\": \"is_type\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"European Rail Agency\", \"type\": \"investigated_by\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"23/12/2021\", \"type\": \"has_date\"},\n",
    "            {\"source\": \"Dublin-Cork Accident\", \"target\": \"16:32\", \"type\": \"has_time\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "    Extract structured entities and output ONLY a JSON object from this railway accident report. Do not provide a summary or comment on the incident.\n",
    "\n",
    "    Return the JSON object with:\n",
    "    - `nodes`: A list of entities, specifically {entities_of_interest}.\n",
    "    - `rels`: A list of relationships linking entities.\n",
    "\n",
    "    Schema example:\n",
    "    {schema_example}\n",
    "\n",
    "    Accident report context:\n",
    "    {text}\n",
    "\n",
    "    JSON:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API URL\n",
    "url = \"http://llama-max-ollama.ai.wu.ac.at/api/generate\"\n",
    "\n",
    "# Define prompt\n",
    "prompt = build_local_prompt(relevant_report_text)\n",
    "\n",
    "# Specify local model\n",
    "MODEL_LOCAL = \"llama3.1:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the payload and query the local model\n",
    "payload = {\n",
    "    \"model\": f\"{MODEL_LOCAL}\",  # Ensure correct model name\n",
    "    \"prompt\": f\"{prompt}\",\n",
    "    \"stream\": False  # If 'raw' is unnecessary, remove it\n",
    "}\n",
    "\n",
    "# Set headers\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Send POST request\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# Handle response\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        data = response.json()  # Parse response JSON\n",
    "        if \"response\" in data:\n",
    "            local_response = textwrap.fill(data[\"response\"], width=80)\n",
    "            print(\"Generated Summary:\\n\")\n",
    "            print(local_response)\n",
    "        else:\n",
    "            print(\"No 'response' key found in the JSON.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON response: {response.text}\")\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    # List of regex patterns to try.\n",
    "    patterns = [\n",
    "        # Pattern for JSON wrapped in a markdown code block:\n",
    "        r'```json\\s*([\\s\\S]*?)\\s*```',\n",
    "        # Fallback pattern: JSON object starting with '{' and ending with '}'\n",
    "        r'({[\\s\\S]*})'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "            # Optionally, remove unwanted control characters.\n",
    "            json_str = re.sub(r'[\\x00-\\x1F]+', '', json_str)\n",
    "            try:\n",
    "                return json.loads(json_str)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"JSON decode error:\", e)\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "local_response = extract_json(local_response)\n",
    "response_dict = {\"model\": MODEL_LOCAL, \"response\": local_response}\n",
    "local_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-model and Iteration Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CSV storage file\n",
    "CSV_FILE = \"pdf_processing_results.csv\"\n",
    "\n",
    "def append_pdf_json_result(pdf_name: str, response_json: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Appends the JSON output of response_json function to a DataFrame.\n",
    "    \n",
    "    - If the PDF has been processed before, it appends a **new row** instead of a new column.\n",
    "    - Prevents duplicate JSON entries for the same iteration.\n",
    "    - Ensures data is **stored in rows**, making querying and analysis easier.\n",
    "\n",
    "    Args:\n",
    "        pdf_name (str): Name of the processed PDF file.\n",
    "        response_json (dict): JSON response from the knowledge extraction process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with the new result.\n",
    "    \"\"\"\n",
    "    # Convert JSON response to a formatted string for easy comparison\n",
    "    json_output = json.dumps(response_json, indent=2)\n",
    "\n",
    "    # Load existing results if the CSV exists\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df = pd.read_csv(CSV_FILE, dtype={\"iteration_number\": int})\n",
    "    else:\n",
    "        # Create an empty DataFrame with the correct schema\n",
    "        df = pd.DataFrame(columns=[\"pdf_name\", \"model_type\", \"iteration_number\", \"json_output\"])\n",
    "\n",
    "    model_type = MODEL_GPT if response_dict.get(\"model\") == MODEL_GPT else MODEL_LOCAL\n",
    "\n",
    "    # Filter for the current PDF's past records\n",
    "    pdf_history = df[df[\"pdf_name\"] == pdf_name]\n",
    "\n",
    "    # Check for duplicates: If this JSON output already exists for the same PDF, skip re-adding\n",
    "    if not pdf_history.empty and json_output in pdf_history[\"json_output\"].values:\n",
    "        print(f\"No changes detected in JSON for {pdf_name}, skipping new entry.\")\n",
    "        return df  # Exit early if it's a duplicate\n",
    "\n",
    "    # Determine new iteration number\n",
    "    iteration_number = pdf_history[\"iteration_number\"].max() + 1 if not pdf_history.empty else 1\n",
    "\n",
    "    # Append new result\n",
    "    new_entry = pd.DataFrame({\"pdf_name\": [pdf_name], \"model_type\": [model_type], \"iteration_number\": [iteration_number], \"json_output\": [json_output]})\n",
    "    df = pd.concat([df, new_entry], ignore_index=True)\n",
    "\n",
    "    # Save back to CSV in **append mode** to avoid full file reads/writes\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "    print(f\"Successfully added {pdf_name} - Iteration {iteration_number} to results!\")\n",
    "    return df\n",
    "\n",
    "# Example execution\n",
    "results_df = append_pdf_json_result(pdf_name, response_dict[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for mapping extracted entities to graph nodes and relationships\n",
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Converts properties to a dictionary for graph storage.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "        return properties\n",
    "    for p in props:\n",
    "        properties[p[\"key\"]] = p[\"value\"]\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Maps extracted entities to graph nodes.\"\"\"\n",
    "    properties = {\"name\": node.id}\n",
    "    return BaseNode(\n",
    "        id=node.id,\n",
    "        type=node.type.capitalize(),\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Maps extracted relationships to graph edges.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j Connection Setup\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\"\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "try:\n",
    "    # Test the connection\n",
    "    with driver.session() as session:\n",
    "        session.run(\"RETURN 1\")\n",
    "    print(\"Connected to Neo4j successfully.\")\n",
    "except AuthError as e:\n",
    "    print(\"Authentication failed. Check your credentials:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear database\n",
    "def clear_neo4j_database():\n",
    "    \"\"\"Delete all nodes and relationships in the Neo4j database.\"\"\"\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    print(\"Neo4j database cleared successfully.\")\n",
    "\n",
    "# Run the function to clear the database\n",
    "clear_neo4j_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_output(df, pdf_name, iteration_number):\n",
    "    \"\"\"\n",
    "    Gets the 'json_output' for the given pdf_name and iteration_number.\n",
    "    Returns an empty dict if there's no match.\n",
    "    \"\"\"\n",
    "    subset = df[\n",
    "        (df[\"pdf_name\"] == pdf_name) &\n",
    "        (df[\"iteration_number\"] == iteration_number)\n",
    "    ]\n",
    "\n",
    "    if subset.empty:\n",
    "        print(\"No match found.\")\n",
    "        return {}\n",
    "\n",
    "    json_str = subset.iloc[0][\"json_output\"]\n",
    "    return json.loads(json_str)\n",
    "\n",
    "# Example usage:\n",
    "pdf_of_choice = \"IE-6305 - 200707_locomotive_224.pdf\"\n",
    "json_to_convert = get_json_output(results_df, pdf_of_choice, 1)\n",
    "print(json.dumps(json_to_convert, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_graph(json_to_convert, source_text):\n",
    "    \"\"\"\n",
    "    Converts extracted JSON into a graph-compatible format with correct entity types.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_node_type(json_data, node_id):\n",
    "        \"\"\"\n",
    "        Helper function to retrieve the correct node type from JSON.\n",
    "        \"\"\"\n",
    "        for node in json_data[\"nodes\"]:\n",
    "            if node[\"id\"] == node_id:\n",
    "                return node[\"type\"]\n",
    "        return \"Unknown\"  # Fallback if type is missing\n",
    "\n",
    "    if not json_to_convert:\n",
    "        print(\"No valid data to convert to a graph.\")\n",
    "        return None\n",
    "\n",
    "    # Convert Nodes\n",
    "    graph_nodes = [map_to_base_node(Node(id=node[\"id\"], type=node[\"type\"])) for node in json_to_convert[\"nodes\"]]\n",
    "\n",
    "    # Convert Relationships (Ensure correct types)\n",
    "    graph_rels = []\n",
    "    for rel in json_to_convert[\"rels\"]:\n",
    "        source_node = Node(id=rel[\"source\"], type=get_node_type(json_to_convert, rel[\"source\"]))\n",
    "        target_node = Node(id=rel[\"target\"], type=get_node_type(json_to_convert, rel[\"target\"]))\n",
    "        graph_rels.append(map_to_base_relationship(Relationship(source=source_node, target=target_node, type=rel[\"type\"])))\n",
    "\n",
    "    return GraphDocument(nodes=graph_nodes, relationships=graph_rels, source=Document(page_content=source_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_neo4j(graph_document):\n",
    "    \"\"\"\n",
    "    Stores extracted knowledge graph into Neo4j with dynamic labels.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Store nodes with dynamic labels\n",
    "        for node in graph_document.nodes:\n",
    "            session.run(f\"\"\"\n",
    "                MERGE (n:{node.type} {{id: $id}})\n",
    "                ON CREATE SET n.name = $name\n",
    "            \"\"\", id=node.id, name=node.id)\n",
    "\n",
    "        # Store relationships\n",
    "        for rel in graph_document.relationships:\n",
    "            session.run(f\"\"\"\n",
    "                MATCH (s {{id: $source}})\n",
    "                MATCH (t {{id: $target}})\n",
    "                MERGE (s)-[r:{rel.type.upper()}]->(t)\n",
    "            \"\"\", source=rel.source.id, target=rel.target.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_railway_accident_report(json_to_convert):\n",
    "    \"\"\"\n",
    "    Converts the JSON to a graph format and stores it in Neo4j.\n",
    "    \"\"\"\n",
    "    print(\"Converting JSON to graph format...\")\n",
    "    graph_document = convert_json_to_graph(json_to_convert, relevant_text)\n",
    "\n",
    "    if graph_document:\n",
    "        print(\"Graph structure created! Storing in Neo4j...\")\n",
    "        store_in_neo4j(graph_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store extracted entities into Neo4j\n",
    "try:\n",
    "    db_result = process_railway_accident_report(json_to_convert)\n",
    "    print(\"Data stored in Neo4j successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to store data in Neo4j:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Neo4j connection\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison against ERAIL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ERAIL DB\n",
    "erail_db = pd.read_excel(\"erail database.xlsx\")\n",
    "\n",
    "erail_db.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime & adjust format\n",
    "erail_db[\"Date of occurrence\"] = erail_db[\"Date of occurrence\"].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# Convert time column to datetime & adjust format\n",
    "erail_db[\"Time of occurrence\"] = pd.to_datetime(erail_db[\"Time of occurrence\"], errors='coerce')\n",
    "erail_db[\"Time of occurrence\"] = erail_db[\"Time of occurrence\"].dt.strftime(\"%H:%M\")\n",
    "\n",
    "erail_db.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the results DataFrame\n",
    "comparison_df = results_df[[\"pdf_name\", \"json_output\"]].copy()\n",
    "\n",
    "# Extract ID from the PDF name\n",
    "comparison_df[\"ERAIL Occurrence\"] = comparison_df[\"pdf_name\"].str.extract(r'(IE-\\d+)')\n",
    "\n",
    "# Sample DataFrame (assuming json_output column contains dictionaries in string format)\n",
    "comparison_df['json_output'] = comparison_df['json_output'].apply(json.loads)  # Convert JSON string to dictionary\n",
    "\n",
    "# Function to extract node data\n",
    "def extract_nodes(json_data):\n",
    "    node_dict = {}\n",
    "    for node in json_data.get(\"nodes\", []):\n",
    "        node_dict[f\"gpt_{node['type']}\"] = node[\"id\"]  # Store ID based on type\n",
    "    return pd.Series(node_dict)  # Convert dictionary to Series for easier DataFrame merging\n",
    "\n",
    "# Apply the function to extract node data\n",
    "nodes_df = comparison_df['json_output'].apply(extract_nodes)\n",
    "\n",
    "# Merge extracted data into original DataFrame\n",
    "comparison_df = pd.concat([comparison_df, nodes_df], axis=1)\n",
    "\n",
    "# Drop the original json_output column if no longer needed\n",
    "comparison_df.drop(columns=[\"json_output\"], inplace=True)\n",
    "\n",
    "# View comparison DataFrame\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the comparison DataFrame with the ERAIL database\n",
    "merged_df = comparison_df.merge(erail_db, on=\"ERAIL Occurrence\", how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GPT extracted date with the ERAIL database (source of truth)\n",
    "merged_df[\"Date Match\"] = np.where(merged_df[\"gpt_Date\"] == merged_df[\"Date of occurrence\"], \"Match\", \"Mismatch\")\n",
    "merged_df[[\"ERAIL Occurrence\", \"gpt_Date\", \"Date of occurrence\", \"Date Match\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GPT extracted time with the ERAIL database (source of truth)\n",
    "merged_df[\"Time Match\"] = np.where(merged_df[\"gpt_Time\"] == merged_df[\"Time of occurrence\"], \"Match\", \"Mismatch\")\n",
    "merged_df[[\"ERAIL Occurrence\", \"gpt_Time\", \"Time of occurrence\", \"Time Match\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GPT extracted country with the ERAIL database (source of truth)\n",
    "merged_df[\"Country Match\"] = np.where(merged_df[\"gpt_Country\"] == merged_df[\"Country\"], \"Match\", \"Mismatch\")\n",
    "merged_df[[\"ERAIL Occurrence\", \"gpt_Country\", \"Country\", \"Country Match\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
